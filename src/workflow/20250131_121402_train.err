
The following have been reloaded with a version change:
  1) python3/3.7.0 => python3/3.9.2

[rank: 42] Seed set to 42
[rank: 46] Seed set to 42
[rank: 0] Seed set to 42
[rank: 37] Seed set to 42
[rank: 58] Seed set to 42
[rank: 53] Seed set to 42
[rank: 29] Seed set to 42
[rank: 33] Seed set to 42
[rank: 48] Seed set to 42
[rank: 11] Seed set to 42
[rank: 19] Seed set to 42
[rank: 63] Seed set to 42
[rank: 5] Seed set to 42
[rank: 9] Seed set to 42
[rank: 13] Seed set to 42
[rank: 30] Seed set to 42
[rank: 26] Seed set to 42
[rank: 31] Seed set to 42
[rank: 6] Seed set to 42
[rank: 28] Seed set to 42
[rank: 10] Seed set to 42
[rank: 20] Seed set to 42
[rank: 4] Seed set to 42
[rank: 15] Seed set to 42
[rank: 14] Seed set to 42
[rank: 12] Seed set to 42
[rank: 18] Seed set to 42
[rank: 21] Seed set to 42
[rank: 22] Seed set to 42
[rank: 23] Seed set to 42
[rank: 1] Seed set to 42
[rank: 8] Seed set to 42
[rank: 3] Seed set to 42
[rank: 16] Seed set to 42
[rank: 17] Seed set to 42
[rank: 7] Seed set to 42
[rank: 2] Seed set to 42
[rank: 32] Seed set to 42
[rank: 36] Seed set to 42
[rank: 40] Seed set to 42
[rank: 44] Seed set to 42
[rank: 34] Seed set to 42
[rank: 39] Seed set to 42
[rank: 41] Seed set to 42
[rank: 45] Seed set to 42
[rank: 55] Seed set to 42
[rank: 35] Seed set to 42
[rank: 38] Seed set to 42
[rank: 43] Seed set to 42
[rank: 47] Seed set to 42
[rank: 52] Seed set to 42
[rank: 56] Seed set to 42
[rank: 54] Seed set to 42
[rank: 51] Seed set to 42
[rank: 57] Seed set to 42
[rank: 50] Seed set to 42
[rank: 59] Seed set to 42
[rank: 49] Seed set to 42
[rank: 25] Seed set to 42
[rank: 60] Seed set to 42
[rank: 24] Seed set to 42
[rank: 62] Seed set to 42
[rank: 27] Seed set to 42
[rank: 61] Seed set to 42
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default
Initializing distributed: GLOBAL_RANK: 40, MEMBER: 41/64
Initializing distributed: GLOBAL_RANK: 32, MEMBER: 33/64
Initializing distributed: GLOBAL_RANK: 60, MEMBER: 61/64
Initializing distributed: GLOBAL_RANK: 48, MEMBER: 49/64
Initializing distributed: GLOBAL_RANK: 20, MEMBER: 21/64
Initializing distributed: GLOBAL_RANK: 28, MEMBER: 29/64
Initializing distributed: GLOBAL_RANK: 56, MEMBER: 57/64
Initializing distributed: GLOBAL_RANK: 12, MEMBER: 13/64
Initializing distributed: GLOBAL_RANK: 36, MEMBER: 37/64
Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/64
Initializing distributed: GLOBAL_RANK: 16, MEMBER: 17/64
Initializing distributed: GLOBAL_RANK: 8, MEMBER: 9/64
Initializing distributed: GLOBAL_RANK: 52, MEMBER: 53/64
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/64
Initializing distributed: GLOBAL_RANK: 44, MEMBER: 45/64
Initializing distributed: GLOBAL_RANK: 24, MEMBER: 25/64
Initializing distributed: GLOBAL_RANK: 39, MEMBER: 40/64
Initializing distributed: GLOBAL_RANK: 37, MEMBER: 38/64
Initializing distributed: GLOBAL_RANK: 38, MEMBER: 39/64
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/64
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/64
Initializing distributed: GLOBAL_RANK: 61, MEMBER: 62/64
Initializing distributed: GLOBAL_RANK: 45, MEMBER: 46/64
Initializing distributed: GLOBAL_RANK: 46, MEMBER: 47/64
Initializing distributed: GLOBAL_RANK: 63, MEMBER: 64/64
Initializing distributed: GLOBAL_RANK: 47, MEMBER: 48/64
Initializing distributed: GLOBAL_RANK: 62, MEMBER: 63/64
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/64
Initializing distributed: GLOBAL_RANK: 42, MEMBER: 43/64
Initializing distributed: GLOBAL_RANK: 21, MEMBER: 22/64
Initializing distributed: GLOBAL_RANK: 9, MEMBER: 10/64
Initializing distributed: GLOBAL_RANK: 41, MEMBER: 42/64
Initializing distributed: GLOBAL_RANK: 54, MEMBER: 55/64
Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/64
Initializing distributed: GLOBAL_RANK: 11, MEMBER: 12/64
Initializing distributed: GLOBAL_RANK: 55, MEMBER: 56/64
Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/64
Initializing distributed: GLOBAL_RANK: 22, MEMBER: 23/64
Initializing distributed: GLOBAL_RANK: 53, MEMBER: 54/64
Initializing distributed: GLOBAL_RANK: 18, MEMBER: 19/64
Initializing distributed: GLOBAL_RANK: 43, MEMBER: 44/64
Initializing distributed: GLOBAL_RANK: 30, MEMBER: 31/64
Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/64
Initializing distributed: GLOBAL_RANK: 19, MEMBER: 20/64
Initializing distributed: GLOBAL_RANK: 23, MEMBER: 24/64
Initializing distributed: GLOBAL_RANK: 29, MEMBER: 30/64
Initializing distributed: GLOBAL_RANK: 27, MEMBER: 28/64
Initializing distributed: GLOBAL_RANK: 10, MEMBER: 11/64
Initializing distributed: GLOBAL_RANK: 17, MEMBER: 18/64
Initializing distributed: GLOBAL_RANK: 31, MEMBER: 32/64
Initializing distributed: GLOBAL_RANK: 51, MEMBER: 52/64
Initializing distributed: GLOBAL_RANK: 15, MEMBER: 16/64
Initializing distributed: GLOBAL_RANK: 13, MEMBER: 14/64
Initializing distributed: GLOBAL_RANK: 14, MEMBER: 15/64
Initializing distributed: GLOBAL_RANK: 26, MEMBER: 27/64
Initializing distributed: GLOBAL_RANK: 50, MEMBER: 51/64
Initializing distributed: GLOBAL_RANK: 59, MEMBER: 60/64
Initializing distributed: GLOBAL_RANK: 33, MEMBER: 34/64
Initializing distributed: GLOBAL_RANK: 34, MEMBER: 35/64
Initializing distributed: GLOBAL_RANK: 57, MEMBER: 58/64
Initializing distributed: GLOBAL_RANK: 49, MEMBER: 50/64
Initializing distributed: GLOBAL_RANK: 58, MEMBER: 59/64
Initializing distributed: GLOBAL_RANK: 35, MEMBER: 36/64
Initializing distributed: GLOBAL_RANK: 25, MEMBER: 26/64
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 64 processes
----------------------------------------------------------------------------------------------------

/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /work2/10214/yu_yao/frontera/deepfaker/src/ckpt exists and is not empty.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name    | Type              | Params | Mode 
------------------------------------------------------
0 | model   | VRES_CNN_64to1024 | 315 M  | train
1 | loss_fn | MSELoss           | 0      | train
------------------------------------------------------
315 M     Trainable params
0         Non-trainable params
315 M     Total params
1,261.912 Total estimated model params size (MB)
52        Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
SLURM auto-requeueing enabled. Setting signal handlers.
/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Errorterminate called after throwing an instance of ''
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Errorterminate called after throwing an instance of ''
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'c10::Errorterminate called after throwing an instance of ''
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Errorc10::Error'
'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'terminate called after throwing an instance of 'c10::Error'
c10::Error'
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b25b629d446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b25b62476e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b25b61b4a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ad3e6698446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ad3e66426e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ad3e65afa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b25b61b4d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b258226ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2582277735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b256b778630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b25b627e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b25b627737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b25b6277529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2e27c90446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2e27c3a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2e27ba7a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b6ab4975446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b6ab491f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b6ab488ca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ad3e65afd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ad3b2669c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ad3b2672735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ad39bb73630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ad3e667969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ad3e667237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ad3e6672529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b256ba3ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b256ba3edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b25b629d446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2e27ba7d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2df3c61c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2df3c6a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2ddd16b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b2e27c7169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2e27c6a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2e27c6a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b6ab488cd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b6a80946c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b6a8094f735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6a69e50630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b6ab495669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b6ab494f37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b6ab494f529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ad39be39a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ad39be39dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ad3e6698446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b25b62476e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b25b61b4a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b25b61b4d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b258226ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2582277735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b256b778630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b25b627e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ac998f99446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ac998f436e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ac998eb0a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b4bfe698446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b4bfe6426e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b4bfe5afa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ac998eb0d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ac964f6ac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ac964f73735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ac94e474630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ac998f7a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ac998f7337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ac998f73529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ab5793aa446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ab5793546e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ab5792c1a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2acc7ce53446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2acc7cdfd6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2acc7cd6aa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b8e52bc1446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b8e52b6b6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b8e52ad8a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6a6a116a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6a6a116dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b6ab4975446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b5bdf4ab446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b5bdf4556e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b5bdf3c2a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ad3e66426e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ad3e65afa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ad3e65afd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ad3b2669c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ad3b2672735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ad39bb73630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b4bfe5afd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b4bca669c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b4bca672735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b4bb3b73630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b4bfe67969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b4bfe67237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b4bfe672529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ac94e73aa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ac94e73adc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b23fe6f1446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b23fe69b6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b23fe608a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b6ab491f6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b6ab488ca18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b6ab488cd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b6a80946c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b6a8094f735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6a69e50630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b5bdf3c2d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b5bab47cc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b5bab485735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b5b94986630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b5bdf48c69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b5bdf48537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b5bdf485529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2ad3e667969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b4bb3e39a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b4bb3e39dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b4bfe698446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b385d531446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b385d4db6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b385d448a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2136632446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b21365dc6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2136549a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2b6ab495669f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b6d34e5f446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b6d34e096e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b6d34d76a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b4bfe6426e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b4bfe5afa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b4bfe5afd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b4bca669c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b4bca672735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b4bb3b73630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b385d448d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b3829502c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b382950b735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b3812a0c630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b385d51269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b385d50b37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b385d50b529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b6ab494f37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b6ab494f529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6a6a116a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6a6a116dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #10: <unknown function> + 0x8c1a78 (0x2b5b94c4ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b5b94c4cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b5bdf4ab446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b6d34d76d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b6d00e30c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b6d00e39735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6cea33a630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b6d34e4069f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b6d34e3937b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b6d34e39529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ad1463fa446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ad1463a46e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ad146311a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2b4bfe67969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2ddd431a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2ddd431dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2e27c90446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b3812cd2a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b3812cd2dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b385d531446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2bab011a0446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2bab0114a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2bab010b7a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6cea600a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6cea600dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b6d34e5f446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ad146311d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ad1123cbc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ad1123d4735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ad0fb8d5630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ad1463db69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ad1463d437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ad1463d4529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b25b627737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b25b6277529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b256ba3ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b256ba3edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2e27c3a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2e27ba7a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2e27ba7d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2df3c61c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2df3c6a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2ddd16b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b385d4db6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b385d448a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b385d448d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b3829502c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b382950b735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b3812a0c630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2136549d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2102603c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b210260c735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b20ebb0d630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b213661369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b213660c37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b213660c529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2bab010b7d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2baacd171c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2baacd17a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2baab667b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2bab0118169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2bab0117a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2bab0117a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b6d34e096e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b6d34d76a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b6d34d76d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b6d00e30c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b6d00e39735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6cea33a630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b8e86360446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b8e8630a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b8e86277a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2b2e27c7169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b385d51269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b20ebdd3a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b20ebdd3dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b23fe608d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b23ca6c2c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b23ca6cb735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b23b3bcc630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b23fe6d269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b23fe6cb37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b23fe6cb529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b8e52ad8d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b8e1eb92c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b8e1eb9b735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b8e0809c630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b8e52ba269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b8e52b9b37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b8e52b9b529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2baab6941a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2baab6941dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2bab011a0446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b5bdf4556e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b5bdf3c2a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b5bdf3c2d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b5bab47cc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b5bab485735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b5b94986630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b6d34e4069f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b8e86277d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b8e52331c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b8e5233a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b8e3b83b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b8e8634169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b8e8633a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b8e8633a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b335cd59446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b335cd036e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b335cc70a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b5c94848446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b5c947f26e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b5c9475fa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b650d8cd446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b650d8776e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b650d7e4a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b948f5a2446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b948f54c6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b948f4b9a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b23b3e92a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b23b3e92dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2acc7cd6ad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2acc48e24c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2acc48e2d735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2acc3232e630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2acc7ce3469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2acc7ce2d37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2acc7ce2d529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b8e08362a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b8e08362dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2bab0114a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2bab010b7a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2bab010b7d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2baacd171c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2baacd17a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2baab667b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b5bdf48c69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b38b7223446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b38b71cd6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b38b713aa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b8e3bb01a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b8e3bb01dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b8e86360446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b335cc70d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b3328d2ac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b3328d33735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b3312234630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b335cd3a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b335cd3337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b335cd33529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b5c9475fd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b5c60819c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b5c60822735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b5c49d23630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b5c9482969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b5c9482237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b5c94822529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ab5792c1d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ab54537bc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ab545384735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ab52e885630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ab57938b69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ab57938437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ab579384529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b650d7e4d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b64d989ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b64d98a7735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b64c2da8630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b650d8ae69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b650d8a737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b650d8a7529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b8d21b30446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b8d21ada6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b8d21a47a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ad315141446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ad3150eb6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ad315058a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2acc325f4a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2acc325f4dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b9c2d071446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b9c2d01b6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b9c2cf88a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2bab0118169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b60a1a93446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b60a1a3d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b60a19aaa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b38b713ad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b38831f4c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b38831fd735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b386c6fe630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b38b720469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b38b71fd37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b38b71fd529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b8e8630a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b8e86277a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b8e86277d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b8e52331c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b8e5233a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b8e3b83b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b33124faa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b33124fadc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b335cd59446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b5c49fe9a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b5c49fe9dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b5c94848446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ab52eb4ba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ab52eb4bdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
frame #10: <unknown function> + 0x8c1a78 (0x2b64c306ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b64c306edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b650d8cd446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b8d21a47d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b8cedb01c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b8cedb0a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b8cd700b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b8d21b1169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b8d21b0a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b8d21b0a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ad315058d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ad2e1112c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ad2e111b735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ad2ca61c630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ad31512269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ad31511b37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ad31511b529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ba8a5efd446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ba8a5ea76e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ba8a5e14a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b9c2cf88d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b9bf9042c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b9bf904b735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b9be254c630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b9c2d05269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b9c2d04b37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b9c2d04b529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b7ea3b2d446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b7ea3ad76e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b7ea3a44a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b60a19aad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b606da64c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b606da6d735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6056f6e630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b60a1a7469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b60a1a6d37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b60a1a6d529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b386c9c4a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b386c9c4dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b38b7223446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ad0fbb9ba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ad0fbb9bdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ad1463fa446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b8e8634169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b335cd036e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b335cc70a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b335cc70d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b3328d2ac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b3328d33735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b3312234630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2f96b62446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2f96b0c6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2f96a79a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b650d8776e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b650d7e4a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b650d7e4d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b64d989ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b64d98a7735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b64c2da8630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b8cd72d1a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b8cd72d1dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b8d21b30446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b948f4b9d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b945b573c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b945b57c735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b9444a7d630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b948f58369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b948f57c37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b948f57c529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ad2ca8e2a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ad2ca8e2dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ad315141446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ba8a5e14d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ba871ecec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ba871ed7735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ba85b3d8630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ba8a5ede69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ba8a5ed737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ba8a5ed7529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b9be2812a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b9be2812dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b9c2d071446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b7ea3a44d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b7e6fafec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b7e6fb07735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b7e59008630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b7ea3b0e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b7ea3b0737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b7ea3b07529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6057234a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6057234dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b60a1a93446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b38b71cd6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b38b713aa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b38b713ad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b38831f4c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b38831fd735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b386c6fe630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ad1463a46e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ad146311a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ad146311d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ad1123cbc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ad1123d4735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ad0fb8d5630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b4bfe67237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b4bfe672529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b4bb3e39a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b4bb3e39dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #7: <unknown function> + 0x6f69f (0x2b335cd3a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2f96a79d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2f62b33c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2f62b3c735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2f4c03d630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b2f96b4369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2f96b3c37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2f96b3c529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b650d8ae69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b8d21ada6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b8d21a47a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b8d21a47d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b8cedb01c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b8cedb0a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b8cd700b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b9444d43a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b9444d43dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ad3150eb6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ad315058a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ad315058d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ad2e1112c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ad2e111b735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ad2ca61c630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ba85b69ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ba85b69edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ba8a5efd446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b9c2d01b6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b9c2cf88a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b9c2cf88d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b9bf9042c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b9bf904b735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b9be254c630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b60a1a3d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b60a19aaa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b60a19aad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b606da64c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b606da6d735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6056f6e630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b38b720469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2ad1463db69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b8e8633a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b8e8633a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b8e3bb01a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b8e3bb01dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2f1ed29446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2f1ecd36e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2f1ec40a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2f4c303a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2f4c303dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2f96b62446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2baaa68b8446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2baaa68626e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2baaa67cfa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2b8d21b1169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b1410a4d446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b14109f76e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b1410964a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2ad31512269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ba8a5ea76e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ba8a5e14a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ba8a5e14d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ba871ecec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ba871ed7735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ba85b3d8630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b9c2d05269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b60a1a7469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ad3e667237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ad3e6672529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ad39be39a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ad39be39dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b0ecfa7e446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b0ecfa286e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b0ecf995a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2f1ec40d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2eeacfac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2eead03735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2ed4204630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b2f1ed0a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2f1ed0337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2f1ed03529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b5c947f26e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b5c9475fa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b5c9475fd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b5c60819c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b5c60822735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b5c49d23630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2baaa67cfd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2baa72889c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2baa72892735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2baa5bd93630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2baaa689969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2baaa689237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2baaa6892529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b561d3f3446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b561d39d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b561d30aa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b19cc31d446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b19cc2c76e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b19cc234a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2ba8a5ede69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b8e52bc1446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b8e52b6b6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b8e52ad8a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b8325d90446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b8325d3a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b8325ca7a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2adc93b74446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2adc93b1e6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2adc93a8ba18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b0ecf995d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b0e9ba4fc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b0e9ba58735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b0e84f59630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b0ecfa5f69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b0ecfa5837b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b0ecfa58529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2ed44caa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2ed44cadc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2f1ed29446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b5c9482969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2baa5c059a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2baa5c059dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2baaa68b8446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b561d30ad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b55e93c4c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b55e93cd735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b55d28ce630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b561d3d469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b561d3cd37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b561d3cd529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b19cc234d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b19982eec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b19982f7735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b19817f8630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b19cc2fe69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b19cc2f737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b19cc2f7529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b41838bb446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b41838656e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b41837d2a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b8e52ad8d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b8e1eb92c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b8e1eb9b735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b8e0809c630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b8e52ba269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b8e52b9b37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b8e52b9b529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2adc93a8bd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2adc5fb45c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2adc5fb4e735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2adc4904f630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2adc93b5569f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2adc93b4e37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2adc93b4e529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b0e8521fa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b0e8521fdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b0ecfa7e446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2f1ecd36e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2f1ec40a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2f1ec40d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2eeacfac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2eead03735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2ed4204630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b6d0544a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b6d053f46e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b6d05361a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b55d2b94a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b55d2b94dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b561d3f3446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b1410964d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b13dca1ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b13dca27735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b13c5f28630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b1410a2e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b1410a2737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b1410a27529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b1981abea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b1981abedc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b19cc31d446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b41837d2d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b414f88cc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b414f895735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b4138d96630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b418389c69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b418389537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b4183895529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b8e08362a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b8e08362dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #10: <unknown function> + 0x8c1a78 (0x2b7e592cea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b7e592cedc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b7ea3b2d446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b0ecfa286e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b0ecf995a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b0ecf995d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b0e9ba4fc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b0e9ba58735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b0e84f59630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b2f1ed0a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b6d05361d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b6cd141bc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b6cd1424735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6cba925630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b6d0542b69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b6d0542437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b6d05424529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2f96b0c6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2f96a79a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2f96a79d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2f62b33c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2f62b3c735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2f4c03d630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b561d39d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b561d30aa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b561d30ad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b55e93c4c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b55e93cd735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b55d28ce630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b13c61eea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b13c61eedc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b1410a4d446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b413905ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b413905cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b41838bb446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ba7eff39446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ba7efee36e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ba7efe50a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b7ea3ad76e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b7ea3a44a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b7ea3a44d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b7e6fafec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b7e6fb07735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b7e59008630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b0ecfa5f69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ad8130e2446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ad81308c6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ad812ff9a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6cbabeba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6cbabebdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b6d0544a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b2f96b4369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b561d3d469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b14109f76e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b1410964a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b1410964d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b13dca1ec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b13dca27735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b13c5f28630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b41838656e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b41837d2a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b41837d2d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b414f88cc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b414f895735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b4138d96630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b98db0ce446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b98db0786e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b98dafe5a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ba7efe50d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ba7bbf0ac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ba7bbf13735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ba7a5414630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ba7eff1a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ba7eff1337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ba7eff13529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b7ea3b0e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b069d5e2446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b069d58c6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b069d4f9a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ad812ff9d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ad7df0b3c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ad7df0bc735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ad7c85bd630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ad8130c369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ad8130bc37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ad8130bc529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b6d053f46e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b6d05361a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b6d05361d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b6cd141bc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b6cd1424735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6cba925630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ae7e8a38446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ae7e89e26e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ae7e894fa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b68b3aa3446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b68b3a4d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b68b39baa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2b1410a2e69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b418389c69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b98dafe5d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b98a709fc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b98a70a8735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b98905a9630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b98db0af69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b98db0a837b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b98db0a8529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aeab9ce7446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aeab9c916e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aeab9bfea18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2adc49315a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2adc49315dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2adc93b74446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ad7c8883a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ad7c8883dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ad8130e2446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b6d0542b69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ae7e894fd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ae7b4a09c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ae7b4a12735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ae79df13630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ae7e8a1969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ae7e8a1237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ae7e8a12529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2afa0d79c446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2afa0d7466e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2afa0d6b3a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2acc7ce53446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2acc7cdfd6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2acc7cd6aa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b989086fa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b989086fdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aeab9bfed02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aea85cb8c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aea85cc1735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2aea6f1c2630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2aeab9cc869f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aeab9cc137b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aeab9cc1529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2adc93b1e6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2adc93a8ba18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2adc93a8bd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2adc5fb45c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2adc5fb4e735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2adc4904f630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ac998f99446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ac998f436e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ac998eb0a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ae79e1d9a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ae79e1d9dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ae7e8a38446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2afa0d6b3d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2af9d976dc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2af9d9776735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2af9c2c77630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2afa0d77d69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2afa0d77637b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2afa0d776529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2acc7cd6ad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2acc48e24c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2acc48e2d735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2acc3232e630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2acc7ce3469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2acc7ce2d37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2acc7ce2d529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aba5de8a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aba5de346e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aba5dda1a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ba7a56daa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ba7a56dadc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ba7eff39446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2aea6f488a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2aea6f488dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aeab9ce7446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2adc93b5569f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ac998eb0d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ac964f6ac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ac964f73735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ac94e474630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ac998f7a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ac998f7337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ac998f73529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ae7e89e26e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ae7e894fa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ae7e894fd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ae7b4a09c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ae7b4a12735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ae79df13630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2af9c2f3da78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2af9c2f3ddc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2afa0d79c446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aba5dda1d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aba29e5bc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aba29e64735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2aba13365630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2aba5de6b69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aba5de6437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aba5de64529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ba7efee36e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ba7efe50a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ba7efe50d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ba7bbf0ac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ba7bbf13735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ba7a5414630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b8325ca7d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b82f1d61c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b82f1d6a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b82db26b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b8325d7169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b8325d6a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b8325d6a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b6d34e3937b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b6d34e39529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6cea600a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6cea600dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #10: <unknown function> + 0x8c1a78 (0x2ac94e73aa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ac94e73adc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #7: <unknown function> + 0x6f69f (0x2ae7e8a1969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2baaa68626e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2baaa67cfa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2baaa67cfd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2baa72889c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2baa72892735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2baa5bd93630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2afa0d7466e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2afa0d6b3a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2afa0d6b3d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2af9d976dc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2af9d9776735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2af9c2c77630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2aba1362ba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2aba1362bdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aba5de8a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2ba7eff1a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aeab9c916e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aeab9bfea18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aeab9bfed02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aea85cb8c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aea85cc1735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2aea6f1c2630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b82db531a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b82db531dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b8325d90446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b38b71fd37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b38b71fd529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b386c9c4a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b386c9c4dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b385d50b37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b385d50b529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b3812cd2a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b3812cd2dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ab5793aa446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ab5793546e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ab5792c1a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2baaa689969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2afa0d77d69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aba5de346e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aba5dda1a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aba5dda1d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aba29e5bc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aba29e64735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2aba13365630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b366017b446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b36601256e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b3660092a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2aeab9cc869f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b8325d3a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b8325ca7a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b8325ca7d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b82f1d61c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b82f1d6a735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b82db26b630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2adc93b4e37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2adc93b4e529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2adc49315a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2adc49315dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ab5792c1d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ab54537bc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ab545384735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ab52e885630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ab57938b69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ab57938437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ab579384529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():    what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b53d3993446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b53d393d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b53d38aaa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2aba5de6b69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b3660092d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b362c14cc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b362c155735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b3615656630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b366015c69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b366015537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b3660155529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2bab0117a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2bab0117a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2baab6941a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2baab6941dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #7: <unknown function> + 0x6f69f (0x2b8325d7169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ab52eb4ba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ab52eb4bdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b53d38aad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b539f964c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b539f96d735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b5388e6e630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b53d397469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b53d396d37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b53d396d529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b19cc2c76e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b19cc234a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b19cc234d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b19982eec4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b19982f7735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b19817f8630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aefb8229446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aefb81d36e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aefb8140a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b361591ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b361591cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b366017b446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b7ea3b0737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b7ea3b07529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b7e592cea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b7e592cedc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2cc1d1c446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2cc1cc66e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2cc1c33a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ad81308c6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ad812ff9a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ad812ff9d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ad7df0b3c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ad7df0bc735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ad7c85bd630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2f96b3c37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2f96b3c529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2f4c303a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2f4c303dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #10: <unknown function> + 0x8c1a78 (0x2b5389134a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b5389134dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b53d3993446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b19cc2fe69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aefb8140d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aef841fac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aef84203735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2aef6d704630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2aefb820a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aefb820337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aefb8203529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b36601256e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b3660092a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b3660092d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b362c14cc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b362c155735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b3615656630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aeab9cc137b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aeab9cc1529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2aea6f488a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2aea6f488dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2cc1c33d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2c8dcedc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2c8dcf6735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2c771f7630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b2cc1cfd69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2cc1cf637b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2cc1cf6529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2ad8130c369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b0d3455a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b0d345046e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b0d34471a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b53d393d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b53d38aaa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b53d38aad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b539f964c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b539f96d735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b5388e6e630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b23fe6f1446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b23fe69b6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b23fe608a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2acc325f4a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2acc325f4dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #10: <unknown function> + 0x8c1a78 (0x2aef6d9caa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2aef6d9cadc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aefb8229446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b366015c69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2c774bda78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2c774bddc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2cc1d1c446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b069d4f9d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b06695b3c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b06695bc735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b0652abd630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b069d5c369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b069d5bc37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b069d5bc529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2e27c6a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2e27c6a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2ddd431a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2ddd431dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b0d34471d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b0d0052bc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b0d00534735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b0ce9a35630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b0d3453b69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b0d3453437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b0d34534529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b53d397469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b23fe608d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b23ca6c2c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b23ca6cb735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b23b3bcc630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b23fe6d269f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b23fe6cb37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b23fe6cb529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ba8a5ed737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ba8a5ed7529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ba85b69ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ba85b69edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b2cc1cc66e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2cc1c33a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2cc1c33d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2c8dcedc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2c8dcf6735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2c771f7630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b0652d83a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b0652d83dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b069d5e2446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b335cd3337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b335cd33529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b33124faa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b33124fadc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #10: <unknown function> + 0x8c1a78 (0x2b0ce9cfba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b0ce9cfbdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b0d3455a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b2136632446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b21365dc6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b2136549a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b68b39bad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b687fa74c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b687fa7d735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6868f7e630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b68b3a8469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b68b3a7d37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b68b3a7d529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b948f5a2446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b948f54c6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b948f4b9a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b23b3e92a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b23b3e92dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b678d6db446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b678d6856e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b678d5f2a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #7: <unknown function> + 0x6f69f (0x2b2cc1cfd69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b069d58c6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b069d4f9a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b069d4f9d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b06695b3c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b06695bc735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b0652abd630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2f1ed0337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2f1ed03529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2ed44caa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2ed44cadc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b0d345046e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b0d34471a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b0d34471d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b0d0052bc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b0d00534735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b0ce9a35630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b2136549d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b2102603c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b210260c735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b20ebb0d630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b213661369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b213660c37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b213660c529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6869244a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6869244dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b68b3aa3446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b948f4b9d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b945b573c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b945b57c735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b9444a7d630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b948f58369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b948f57c37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b948f57c529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ad31511b37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ad31511b529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ad2ca8e2a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ad2ca8e2dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b678d5f2d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b67596acc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b67596b5735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6742bb6630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b678d6bc69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b678d6b537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b678d6b5529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aefb81d36e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aefb8140a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aefb8140d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aef841fac4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aef84203735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2aef6d704630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b9c2d04b37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b9c2d04b529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b9be2812a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b9be2812dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b5bdf48537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b5bdf485529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b5b94c4ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b5b94c4cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #7: <unknown function> + 0x6f69f (0x2b069d5c369f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ad8130bc37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ad8130bc529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ad7c8883a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ad7c8883dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #7: <unknown function> + 0x6f69f (0x2b0d3453b69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b20ebdd3a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b20ebdd3dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b68b3a4d6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b68b39baa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b68b39bad02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b687fa74c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b687fa7d735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6868f7e630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b9444d43a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b9444d43dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b19cc2f737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b19cc2f7529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b1981abea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b1981abedc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #10: <unknown function> + 0x8c1a78 (0x2b6742e7ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6742e7cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b678d6db446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2aefb820a69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ba7eff1337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ba7eff13529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ba7a56daa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ba7a56dadc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b60a1a6d37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b60a1a6d529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6057234a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6057234dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b227e147446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b227e0f16e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b227e05ea18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ae7e8a1237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ae7e8a12529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ae79e1d9a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ae79e1d9dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b650d8a737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b650d8a7529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b64c306ea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b64c306edc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #7: <unknown function> + 0x6f69f (0x2b68b3a8469f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b1410a2737b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b1410a27529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b13c61eea78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b13c61eedc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2af5f40d8446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2af5f40826e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2af5f3fefa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b678d6856e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b678d5f2a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b678d5f2d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b67596acc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b67596b5735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b6742bb6630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b366015537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b3660155529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b361591ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b361591cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b8325d6a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b8325d6a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b82db531a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b82db531dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b227e05ed02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b224a118c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b224a121735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2233622630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b227e12869f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b227e12137b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b227e121529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b5c9482237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b5c94822529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b5c49fe9a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b5c49fe9dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b0d3453437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b0d34534529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b0ce9cfba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b0ce9cfbdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2baaa689237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2baaa6892529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2baa5c059a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2baa5c059dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b4ee530e446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b4ee52b86e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b4ee5225a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2afa0d77637b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2afa0d776529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2af9c2f3da78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2af9c2f3ddc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2af5f3fefd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2af5c00a9c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2af5c00b2735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2af5a95b3630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2af5f40b969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2af5f40b237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2af5f40b2529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b678d6bc69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b2cc1cf637b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b2cc1cf6529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b2c774bda78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b2c774bddc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #10: <unknown function> + 0x8c1a78 (0x2b22338e8a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b22338e8dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b227e147446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b6d0542437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b6d05424529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6cbabeba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6cbabebdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b53d396d37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b53d396d529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b5389134a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b5389134dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b4ee5225d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b4eb12dfc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b4eb12e8735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b4e9a7e9630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b4ee52ef69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b4ee52e837b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b4ee52e8529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aca05a86446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aca05a306e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aca0599da18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #10: <unknown function> + 0x8c1a78 (0x2af5a9879a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2af5a9879dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2af5f40d8446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b418389537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b4183895529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b413905ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b413905cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aacc1294446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aacc123e6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aacc11aba18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b227e0f16e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b227e05ea18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b227e05ed02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b224a118c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b224a121735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b2233622630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b4e9aaafa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b4e9aaafdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b4ee530e446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aca0599dd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ac9d1a57c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ac9d1a60735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ac9baf61630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2aca05a6769f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aca05a6037b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aca05a60529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2af5f40826e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2af5f3fefa18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2af5f3fefd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2af5c00a9c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2af5c00b2735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2af5a95b3630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b678d6b537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b678d6b5529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6742e7ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6742e7cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aacc11abd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aac8d265c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aac8d26e735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2aac7676f630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2aacc127569f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aacc126e37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aacc126e529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2b227e12869f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b4ee52b86e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b4ee5225a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b4ee5225d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b4eb12dfc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b4eb12e8735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b4e9a7e9630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ac9bb227a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ac9bb227dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aca05a86446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2af5f40b969f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2aac76a35a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2aac76a35dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2aacc1294446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ad1463d437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ad1463d4529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ad0fbb9ba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ad0fbb9bdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #7: <unknown function> + 0x6f69f (0x2b4ee52ef69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aca05a306e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aca0599da18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aca0599dd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ac9d1a57c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ac9d1a60735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ac9baf61630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2af5f40b237b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2af5f40b2529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2af5a9879a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2af5a9879dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2aacc123e6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2aacc11aba18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2aacc11abd02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aac8d265c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aac8d26e735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2aac7676f630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b0ecfa5837b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b0ecfa58529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b0e8521fa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b0e8521fdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b8d21b0a37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b8d21b0a529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b8cd72d1a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b8cd72d1dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #7: <unknown function> + 0x6f69f (0x2aca05a6769f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #7: <unknown function> + 0x6f69f (0x2aacc127569f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b069d5bc37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b069d5bc529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b0652d83a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b0652d83dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b561d3cd37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b561d3cd529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b55d2b94a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b55d2b94dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aca05a6037b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aca05a60529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ac9bb227a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ac9bb227dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b98db0ce446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b98db0786e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b98dafe5a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b227e12137b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b227e121529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b22338e8a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b22338e8dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b68b3a7d37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b68b3a7d529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b6869244a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b6869244dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b98dafe5d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b98a709fc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b98a70a8735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b98905a9630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b98db0af69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b98db0a837b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b98db0a8529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b4ee52e837b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b4ee52e8529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b4e9aaafa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b4e9aaafdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #10: <unknown function> + 0x8c1a78 (0x2b989086fa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b989086fdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aba5de6437b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aba5de64529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2aba1362ba78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2aba1362bdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aefb820337b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aefb8203529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2aef6d9caa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2aef6d9cadc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2aacc126e37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2aacc126e529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2aac76a35a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2aac76a35dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>

terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
terminate called after throwing an instance of 'c10::Error'
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2acee5be0446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2acee5b8a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2acee5af7a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2acee5af7d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aceb1bb1c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aceb1bba735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ace9b0bb630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2acee5bc169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2acee5bba37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2acee5bba529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ace9b381a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ace9b381dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2acee5be0446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2acee5b8a6e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2acee5af7a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2acee5af7d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2aceb1bb1c4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2aceb1bba735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ace9b0bb630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2acee5bc169f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ba88b7fb446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ba88b7a56e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ba88b712a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ba88b712d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ba8577ccc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ba8577d5735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ba840cd6630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ba88b7dc69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ba88b7d537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ba88b7d5529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ba840f9ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ba840f9cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2ba88b7fb446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2ba88b7a56e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2ba88b712a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2ba88b712d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2ba8577ccc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2ba8577d5735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2ba840cd6630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2ba88b7dc69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b6218c0e446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b6218bb86e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b6218b25a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b6218b25d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b61e4bdfc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b61e4be8735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b61ce0e9630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b6218bef69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b6218be837b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b6218be8529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b61ce3afa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b61ce3afdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b6218c0e446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b6218bb86e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b6218b25a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b6218b25d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b61e4bdfc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b61e4be8735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b61ce0e9630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b6218bef69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b26c897b446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b26c89256e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b26c8892a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b26c8892d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b269494cc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2694955735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b267de56630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b26c895c69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b26c895537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b26c8955529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b267e11ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b267e11cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>
  what():  CUDA error: initialization error
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Exception raised from c10_cuda_check_implementation at ../c10/cuda/CUDAException.cpp:43 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b26c897b446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::string const&) + 0x64 (0x2b26c89256e4 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #2: c10::cuda::c10_cuda_check_implementation(int, char const*, char const*, int, bool) + 0x118 (0x2b26c8892a18 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #3: c10::cuda::ExchangeDevice(signed char) + 0x92 (0x2b26c8892d02 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10_cuda.so)
frame #4: <unknown function> + 0x1021c4a (0x2b269494cc4a in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x102a735 (0x2b2694955735 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0x5fb630 (0x2b267de56630 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x6f69f (0x2b26c895c69f in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2acee5bba37b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2acee5bba529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ace9b381a78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ace9b381dc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2ba88b7d537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2ba88b7d5529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2ba840f9ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2ba840f9cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b6218be837b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b6218be8529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b61ce3afa78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b61ce3afdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


frame #8: c10::TensorImpl::~TensorImpl() + 0x21b (0x2b26c895537b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #9: c10::TensorImpl::~TensorImpl() + 0x9 (0x2b26c8955529 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #10: <unknown function> + 0x8c1a78 (0x2b267e11ca78 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
frame #11: THPVariable_subclass_dealloc(_object*) + 0x2c6 (0x2b267e11cdc6 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_python.so)
<omitting python frames>


[rank22]: Traceback (most recent call last):
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank22]:     data = self._data_queue.get(timeout=timeout)
[rank22]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank22]:     self.not_empty.wait(remaining)
[rank22]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank22]:     gotit = waiter.acquire(True, timeout)
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank22]:     _error_if_any_worker_fails()
[rank22]: RuntimeError: DataLoader worker (pid 512) is killed by signal: Aborted. 

[rank22]: The above exception was the direct cause of the following exception:

[rank22]: Traceback (most recent call last):
[rank22]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank53]: Traceback (most recent call last):
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank53]:     data = self._data_queue.get(timeout=timeout)
[rank53]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank53]:     self.not_empty.wait(remaining)
[rank53]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank53]:     gotit = waiter.acquire(True, timeout)
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank53]:     _error_if_any_worker_fails()
[rank53]: RuntimeError: DataLoader worker (pid 453) is killed by signal: Aborted. 

[rank53]: The above exception was the direct cause of the following exception:

[rank53]: Traceback (most recent call last):
[rank53]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank22]:     main(
[rank22]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank22]:     trainer.fit(
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank22]:     call._call_and_handle_interrupt(
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank22]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank22]:     return function(*args, **kwargs)
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank22]:     self._run(model, ckpt_path=ckpt_path)
[rank53]:     main(
[rank53]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank53]:     trainer.fit(
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank53]:     call._call_and_handle_interrupt(
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank53]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank53]:     return function(*args, **kwargs)
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank53]:     self._run(model, ckpt_path=ckpt_path)
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank22]:     results = self._run_stage()
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank22]:     self.fit_loop.run()
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank22]:     self.advance()
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank22]:     self.epoch_loop.run(self._data_fetcher)
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank22]:     self.advance(data_fetcher)
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank53]:     results = self._run_stage()
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank53]:     self.fit_loop.run()
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank53]:     self.advance()
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank53]:     self.epoch_loop.run(self._data_fetcher)
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank53]:     self.advance(data_fetcher)
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank51]: Traceback (most recent call last):
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank51]:     data = self._data_queue.get(timeout=timeout)
[rank51]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank51]:     self.not_empty.wait(remaining)
[rank51]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank51]:     gotit = waiter.acquire(True, timeout)
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank51]:     _error_if_any_worker_fails()
[rank51]: RuntimeError: DataLoader worker (pid 15626) is killed by signal: Aborted. 

[rank51]: The above exception was the direct cause of the following exception:

[rank51]: Traceback (most recent call last):
[rank51]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank22]:     batch, _, __ = next(data_fetcher)
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank22]:     batch = super().__next__()
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank22]:     batch = next(self.iterator)
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank22]:     out = next(self._iterator)
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank22]:     out[i] = next(self.iterators[i])
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank22]:     data = self._next_data()
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank22]:     idx, data = self._get_data()
[rank20]: Traceback (most recent call last):
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank20]:     data = self._data_queue.get(timeout=timeout)
[rank20]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank20]:     self.not_empty.wait(remaining)
[rank20]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank20]:     gotit = waiter.acquire(True, timeout)
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank20]:     _error_if_any_worker_fails()
[rank20]: RuntimeError: DataLoader worker (pid 505) is killed by signal: Aborted. 

[rank20]: The above exception was the direct cause of the following exception:

[rank20]: Traceback (most recent call last):
[rank20]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank53]:     batch, _, __ = next(data_fetcher)
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank53]:     batch = super().__next__()
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank53]:     batch = next(self.iterator)
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank53]:     out = next(self._iterator)
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank53]:     out[i] = next(self.iterators[i])
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank53]:     data = self._next_data()
[rank51]:     main(
[rank51]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank51]:     trainer.fit(
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank51]:     call._call_and_handle_interrupt(
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank51]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank51]:     return function(*args, **kwargs)
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank51]:     self._run(model, ckpt_path=ckpt_path)
[rank8]: Traceback (most recent call last):
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank8]:     data = self._data_queue.get(timeout=timeout)
[rank8]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank8]:     self.not_empty.wait(remaining)
[rank8]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank8]:     gotit = waiter.acquire(True, timeout)
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank8]:     _error_if_any_worker_fails()
[rank8]: RuntimeError: DataLoader worker (pid 10362) is killed by signal: Aborted. 

[rank8]: The above exception was the direct cause of the following exception:

[rank8]: Traceback (most recent call last):
[rank8]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank8]:     main(
[rank20]:     main(
[rank20]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank20]:     trainer.fit(
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank20]:     call._call_and_handle_interrupt(
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank20]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank20]:     return function(*args, **kwargs)
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank20]:     self._run(model, ckpt_path=ckpt_path)
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank53]:     idx, data = self._get_data()
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank51]:     results = self._run_stage()
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank51]:     self.fit_loop.run()
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank51]:     self.advance()
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank51]:     self.epoch_loop.run(self._data_fetcher)
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank51]:     self.advance(data_fetcher)
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank20]:     results = self._run_stage()
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank20]:     self.fit_loop.run()
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank20]:     self.advance()
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank20]:     self.epoch_loop.run(self._data_fetcher)
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank20]:     self.advance(data_fetcher)
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank53]:     success, data = self._try_get_data()
[rank53]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank53]:     raise RuntimeError(
[rank53]: RuntimeError: DataLoader worker (pid(s) 453, 460) exited unexpectedly
[rank8]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank8]:     trainer.fit(
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank8]:     call._call_and_handle_interrupt(
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank8]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank8]:     return function(*args, **kwargs)
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank8]:     self._run(model, ckpt_path=ckpt_path)
[rank20]:     batch, _, __ = next(data_fetcher)
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank20]:     batch = super().__next__()
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank20]:     batch = next(self.iterator)
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank20]:     out = next(self._iterator)
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank20]:     out[i] = next(self.iterators[i])
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank20]:     data = self._next_data()
[rank54]: Traceback (most recent call last):
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank54]:     data = self._data_queue.get(timeout=timeout)
[rank54]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank54]:     self.not_empty.wait(remaining)
[rank54]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank54]:     gotit = waiter.acquire(True, timeout)
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank54]:     _error_if_any_worker_fails()
[rank54]: RuntimeError: DataLoader worker (pid 461) is killed by signal: Aborted. 

[rank54]: The above exception was the direct cause of the following exception:

[rank54]: Traceback (most recent call last):
[rank54]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank8]:     results = self._run_stage()
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank8]:     self.fit_loop.run()
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank8]:     self.advance()
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank8]:     self.epoch_loop.run(self._data_fetcher)
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank8]:     self.advance(data_fetcher)
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank20]:     idx, data = self._get_data()
[rank54]:     main(
[rank54]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank54]:     trainer.fit(
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank54]:     call._call_and_handle_interrupt(
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank54]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank54]:     return function(*args, **kwargs)
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank54]:     self._run(model, ckpt_path=ckpt_path)
[rank51]:     batch, _, __ = next(data_fetcher)
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank51]:     batch = super().__next__()
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank51]:     batch = next(self.iterator)
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank51]:     out = next(self._iterator)
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank51]:     out[i] = next(self.iterators[i])
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank51]:     data = self._next_data()
[rank8]:     batch, _, __ = next(data_fetcher)
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank8]:     batch = super().__next__()
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank8]:     batch = next(self.iterator)
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank8]:     out = next(self._iterator)
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank8]:     out[i] = next(self.iterators[i])
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank8]:     data = self._next_data()
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank22]:     success, data = self._try_get_data()
[rank22]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank22]:     raise RuntimeError(
[rank22]: RuntimeError: DataLoader worker (pid(s) 507, 512) exited unexpectedly
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank54]:     results = self._run_stage()
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank54]:     self.fit_loop.run()
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank54]:     self.advance()
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank54]:     self.epoch_loop.run(self._data_fetcher)
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank54]:     self.advance(data_fetcher)
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank51]:     idx, data = self._get_data()
[rank60]: Traceback (most recent call last):
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank60]:     data = self._data_queue.get(timeout=timeout)
[rank60]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank60]:     self.not_empty.wait(remaining)
[rank60]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank60]:     gotit = waiter.acquire(True, timeout)
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank60]:     _error_if_any_worker_fails()
[rank60]: RuntimeError: DataLoader worker (pid 10810) is killed by signal: Aborted. 

[rank60]: The above exception was the direct cause of the following exception:

[rank60]: Traceback (most recent call last):
[rank60]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank8]:     idx, data = self._get_data()
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank20]:     success, data = self._try_get_data()
[rank20]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank20]:     raise RuntimeError(
[rank20]: RuntimeError: DataLoader worker (pid(s) 505, 509) exited unexpectedly
[rank54]:     batch, _, __ = next(data_fetcher)
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank54]:     batch = super().__next__()
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank54]:     batch = next(self.iterator)
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank54]:     out = next(self._iterator)
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank54]:     out[i] = next(self.iterators[i])
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank54]:     data = self._next_data()
[rank48]: Traceback (most recent call last):
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank48]:     data = self._data_queue.get(timeout=timeout)
[rank48]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank48]:     self.not_empty.wait(remaining)
[rank48]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank48]:     gotit = waiter.acquire(True, timeout)
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank48]:     _error_if_any_worker_fails()
[rank48]: RuntimeError: DataLoader worker (pid 15628) is killed by signal: Aborted. 

[rank48]: The above exception was the direct cause of the following exception:

[rank48]: Traceback (most recent call last):
[rank48]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank60]:     main(
[rank60]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank60]:     trainer.fit(
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank60]:     call._call_and_handle_interrupt(
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank60]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank60]:     return function(*args, **kwargs)
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank60]:     self._run(model, ckpt_path=ckpt_path)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank1]:     data = self._data_queue.get(timeout=timeout)
[rank1]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank1]:     self.not_empty.wait(remaining)
[rank1]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank1]:     gotit = waiter.acquire(True, timeout)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 70, in handler
[rank1]:     def handler(signum, frame):
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank1]:     _error_if_any_worker_fails()
[rank1]: RuntimeError: DataLoader worker (pid 4402) is killed by signal: Aborted. 

[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank8]:     success, data = self._try_get_data()
[rank8]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank8]:     raise RuntimeError(
[rank8]: RuntimeError: DataLoader worker (pid(s) 10362, 10370) exited unexpectedly
[rank21]: Traceback (most recent call last):
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank21]:     data = self._data_queue.get(timeout=timeout)
[rank21]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank21]:     self.not_empty.wait(remaining)
[rank21]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank21]:     gotit = waiter.acquire(True, timeout)
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank21]:     _error_if_any_worker_fails()
[rank21]: RuntimeError: DataLoader worker (pid 506) is killed by signal: Aborted. 

[rank21]: The above exception was the direct cause of the following exception:

[rank21]: Traceback (most recent call last):
[rank21]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank54]:     idx, data = self._get_data()
[rank48]:     main(
[rank48]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank48]:     trainer.fit(
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank48]:     call._call_and_handle_interrupt(
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank48]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank48]:     return function(*args, **kwargs)
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank48]:     self._run(model, ckpt_path=ckpt_path)
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank60]:     results = self._run_stage()
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank60]:     self.fit_loop.run()
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank60]:     self.advance()
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank60]:     self.epoch_loop.run(self._data_fetcher)
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank60]:     self.advance(data_fetcher)
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank1]:     main(
[rank1]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank1]:     trainer.fit(
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank1]:     call._call_and_handle_interrupt(
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank1]:     return function(*args, **kwargs)
[rank11]: Traceback (most recent call last):
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank11]:     data = self._data_queue.get(timeout=timeout)
[rank11]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank11]:     self.not_empty.wait(remaining)
[rank11]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank11]:     gotit = waiter.acquire(True, timeout)
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank11]:     _error_if_any_worker_fails()
[rank11]: RuntimeError: DataLoader worker (pid 10365) is killed by signal: Aborted. 

[rank11]: The above exception was the direct cause of the following exception:

[rank11]: Traceback (most recent call last):
[rank11]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank21]:     main(
[rank21]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank21]:     trainer.fit(
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank21]:     call._call_and_handle_interrupt(
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank21]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank21]:     return function(*args, **kwargs)
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank21]:     self._run(model, ckpt_path=ckpt_path)
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank54]:     success, data = self._try_get_data()
[rank54]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank54]:     raise RuntimeError(
[rank54]: RuntimeError: DataLoader worker (pid(s) 456, 461) exited unexpectedly
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank48]:     results = self._run_stage()
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank48]:     self.fit_loop.run()
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank48]:     self.advance()
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank48]:     self.epoch_loop.run(self._data_fetcher)
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank48]:     self.advance(data_fetcher)
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank60]:     batch, _, __ = next(data_fetcher)
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank60]:     batch = super().__next__()
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank60]:     batch = next(self.iterator)
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank60]:     out = next(self._iterator)
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank60]:     out[i] = next(self.iterators[i])
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank60]:     data = self._next_data()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank1]:     self._run(model, ckpt_path=ckpt_path)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank1]:     results = self._run_stage()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank1]:     self.fit_loop.run()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank1]:     self.advance()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank1]:     self.epoch_loop.run(self._data_fetcher)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank21]:     results = self._run_stage()
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank21]:     self.fit_loop.run()
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank21]:     self.advance()
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank21]:     self.epoch_loop.run(self._data_fetcher)
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank21]:     self.advance(data_fetcher)
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank44]: Traceback (most recent call last):
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank44]:     data = self._data_queue.get(timeout=timeout)
[rank44]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank44]:     self.not_empty.wait(remaining)
[rank44]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank44]:     gotit = waiter.acquire(True, timeout)
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank44]:     _error_if_any_worker_fails()
[rank44]: RuntimeError: DataLoader worker (pid 9220) is killed by signal: Aborted. 

[rank44]: The above exception was the direct cause of the following exception:

[rank44]: Traceback (most recent call last):
[rank44]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank55]: Traceback (most recent call last):
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank55]:     data = self._data_queue.get(timeout=timeout)
[rank55]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank55]:     self.not_empty.wait(remaining)
[rank55]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank55]:     gotit = waiter.acquire(True, timeout)
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank55]:     _error_if_any_worker_fails()
[rank55]: RuntimeError: DataLoader worker (pid 455) is killed by signal: Aborted. 

[rank55]: The above exception was the direct cause of the following exception:

[rank55]: Traceback (most recent call last):
[rank55]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank48]:     batch, _, __ = next(data_fetcher)
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank48]:     batch = super().__next__()
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank48]:     batch = next(self.iterator)
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank48]:     out = next(self._iterator)
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank48]:     out[i] = next(self.iterators[i])
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank48]:     data = self._next_data()
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank60]:     idx, data = self._get_data()
[rank1]:     self.advance(data_fetcher)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank1]:     batch, _, __ = next(data_fetcher)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank1]:     batch = super().__next__()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank1]:     batch = next(self.iterator)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank1]:     out = next(self._iterator)
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank1]:     out[i] = next(self.iterators[i])
[rank21]:     batch, _, __ = next(data_fetcher)
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank21]:     batch = super().__next__()
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank21]:     batch = next(self.iterator)
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank21]:     out = next(self._iterator)
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank21]:     out[i] = next(self.iterators[i])
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank21]:     data = self._next_data()
[rank37]: Traceback (most recent call last):
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank37]:     data = self._data_queue.get(timeout=timeout)
[rank37]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank37]:     self.not_empty.wait(remaining)
[rank37]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank37]:     gotit = waiter.acquire(True, timeout)
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank37]:     _error_if_any_worker_fails()
[rank37]: RuntimeError: DataLoader worker (pid 16971) is killed by signal: Aborted. 

[rank37]: The above exception was the direct cause of the following exception:

[rank37]: Traceback (most recent call last):
[rank37]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank44]:     main(
[rank44]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank44]:     trainer.fit(
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank44]:     call._call_and_handle_interrupt(
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank44]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank44]:     return function(*args, **kwargs)
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank44]:     self._run(model, ckpt_path=ckpt_path)
[rank55]:     main(
[rank55]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank55]:     trainer.fit(
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank55]:     call._call_and_handle_interrupt(
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank55]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank55]:     return function(*args, **kwargs)
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank55]:     self._run(model, ckpt_path=ckpt_path)
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank48]:     idx, data = self._get_data()
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank60]:     success, data = self._try_get_data()
[rank60]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank60]:     raise RuntimeError(
[rank60]: RuntimeError: DataLoader worker (pid(s) 10810, 10813) exited unexpectedly
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank1]:     data = self._next_data()
[rank11]:     main(
[rank11]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank11]:     trainer.fit(
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank11]:     call._call_and_handle_interrupt(
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank11]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank11]:     return function(*args, **kwargs)
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank11]:     self._run(model, ckpt_path=ckpt_path)
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank21]:     idx, data = self._get_data()
[rank37]:     main(
[rank37]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank37]:     trainer.fit(
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank37]:     call._call_and_handle_interrupt(
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank37]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank37]:     return function(*args, **kwargs)
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank37]:     self._run(model, ckpt_path=ckpt_path)
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank44]:     results = self._run_stage()
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank44]:     self.fit_loop.run()
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank44]:     self.advance()
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank44]:     self.epoch_loop.run(self._data_fetcher)
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank44]:     self.advance(data_fetcher)
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank55]:     results = self._run_stage()
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank55]:     self.fit_loop.run()
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank55]:     self.advance()
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank55]:     self.epoch_loop.run(self._data_fetcher)
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank55]:     self.advance(data_fetcher)
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank50]: Traceback (most recent call last):
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank50]:     data = self._data_queue.get(timeout=timeout)
[rank50]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank50]:     self.not_empty.wait(remaining)
[rank50]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank50]:     gotit = waiter.acquire(True, timeout)
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank50]:     _error_if_any_worker_fails()
[rank50]: RuntimeError: DataLoader worker (pid 15629) is killed by signal: Aborted. 

[rank50]: The above exception was the direct cause of the following exception:

[rank50]: Traceback (most recent call last):
[rank50]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank1]:     idx, data = self._get_data()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank1]:     success, data = self._try_get_data()
[rank1]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank1]:     raise RuntimeError(
[rank1]: RuntimeError: DataLoader worker (pid(s) 4402, 4414) exited unexpectedly
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank11]:     results = self._run_stage()
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank11]:     self.fit_loop.run()
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank11]:     self.advance()
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank11]:     self.epoch_loop.run(self._data_fetcher)
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank11]:     self.advance(data_fetcher)
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank17]: Traceback (most recent call last):
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank17]:     data = self._data_queue.get(timeout=timeout)
[rank17]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank17]:     self.not_empty.wait(remaining)
[rank17]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank17]:     gotit = waiter.acquire(True, timeout)
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank17]:     _error_if_any_worker_fails()
[rank17]: RuntimeError: DataLoader worker (pid 31611) is killed by signal: Aborted. 

[rank17]: The above exception was the direct cause of the following exception:

[rank17]: Traceback (most recent call last):
[rank17]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank21]:     success, data = self._try_get_data()
[rank21]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank21]:     raise RuntimeError(
[rank21]: RuntimeError: DataLoader worker (pid(s) 506, 511) exited unexpectedly
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank37]:     results = self._run_stage()
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank37]:     self.fit_loop.run()
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank37]:     self.advance()
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank37]:     self.epoch_loop.run(self._data_fetcher)
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank37]:     self.advance(data_fetcher)
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank44]:     batch, _, __ = next(data_fetcher)
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank44]:     batch = super().__next__()
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank44]:     batch = next(self.iterator)
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank44]:     out = next(self._iterator)
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank44]:     out[i] = next(self.iterators[i])
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank44]:     data = self._next_data()
[rank55]:     batch, _, __ = next(data_fetcher)
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank55]:     batch = super().__next__()
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank55]:     batch = next(self.iterator)
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank55]:     out = next(self._iterator)
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank55]:     out[i] = next(self.iterators[i])
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank55]:     data = self._next_data()
[rank50]:     main(
[rank50]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank50]:     trainer.fit(
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank50]:     call._call_and_handle_interrupt(
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank50]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank50]:     return function(*args, **kwargs)
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank50]:     self._run(model, ckpt_path=ckpt_path)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank3]:     data = self._data_queue.get(timeout=timeout)
[rank3]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank3]:     self.not_empty.wait(remaining)
[rank3]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank3]:     gotit = waiter.acquire(True, timeout)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank3]:     _error_if_any_worker_fails()
[rank3]: RuntimeError: DataLoader worker (pid 4404) is killed by signal: Aborted. 

[rank3]: The above exception was the direct cause of the following exception:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank3]:     main(
[rank11]:     batch, _, __ = next(data_fetcher)
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank11]:     batch = super().__next__()
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank11]:     batch = next(self.iterator)
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank11]:     out = next(self._iterator)
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank11]:     out[i] = next(self.iterators[i])
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank11]:     data = self._next_data()
[rank17]:     main(
[rank17]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank17]:     trainer.fit(
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank17]:     call._call_and_handle_interrupt(
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank17]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank17]:     return function(*args, **kwargs)
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank17]:     self._run(model, ckpt_path=ckpt_path)
[rank23]: Traceback (most recent call last):
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank23]:     data = self._data_queue.get(timeout=timeout)
[rank23]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank23]:     self.not_empty.wait(remaining)
[rank23]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank23]:     gotit = waiter.acquire(True, timeout)
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank23]:     _error_if_any_worker_fails()
[rank23]: RuntimeError: DataLoader worker (pid 503) is killed by signal: Aborted. 

[rank23]: The above exception was the direct cause of the following exception:

[rank23]: Traceback (most recent call last):
[rank23]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank37]:     batch, _, __ = next(data_fetcher)
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank37]:     batch = super().__next__()
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank37]:     batch = next(self.iterator)
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank37]:     out = next(self._iterator)
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank37]:     out[i] = next(self.iterators[i])
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank37]:     data = self._next_data()
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank44]:     idx, data = self._get_data()
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank55]:     idx, data = self._get_data()
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank50]:     results = self._run_stage()
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank50]:     self.fit_loop.run()
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank50]:     self.advance()
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank50]:     self.epoch_loop.run(self._data_fetcher)
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank50]:     self.advance(data_fetcher)
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank3]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank3]:     trainer.fit(
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank3]:     call._call_and_handle_interrupt(
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank3]:     return function(*args, **kwargs)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank3]:     self._run(model, ckpt_path=ckpt_path)
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank11]:     idx, data = self._get_data()
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank17]:     results = self._run_stage()
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank17]:     self.fit_loop.run()
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank17]:     self.advance()
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank17]:     self.epoch_loop.run(self._data_fetcher)
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank17]:     self.advance(data_fetcher)
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank23]:     main(
[rank23]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank23]:     trainer.fit(
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank23]:     call._call_and_handle_interrupt(
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank23]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank23]:     return function(*args, **kwargs)
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank23]:     self._run(model, ckpt_path=ckpt_path)
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank37]:     idx, data = self._get_data()
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank44]:     success, data = self._try_get_data()
[rank44]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank44]:     raise RuntimeError(
[rank44]: RuntimeError: DataLoader worker (pid(s) 9220, 9225) exited unexpectedly
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank55]:     success, data = self._try_get_data()
[rank55]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank55]:     raise RuntimeError(
[rank55]: RuntimeError: DataLoader worker (pid(s) 455, 457) exited unexpectedly
[rank50]:     batch, _, __ = next(data_fetcher)
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank50]:     batch = super().__next__()
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank50]:     batch = next(self.iterator)
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank50]:     out = next(self._iterator)
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank50]:     out[i] = next(self.iterators[i])
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank50]:     data = self._next_data()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank3]:     results = self._run_stage()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank3]:     self.fit_loop.run()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank3]:     self.advance()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank3]:     self.epoch_loop.run(self._data_fetcher)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank3]:     self.advance(data_fetcher)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank11]:     success, data = self._try_get_data()
[rank11]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank11]:     raise RuntimeError(
[rank11]: RuntimeError: DataLoader worker (pid(s) 10365, 10366) exited unexpectedly
[rank17]:     batch, _, __ = next(data_fetcher)
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank17]:     batch = super().__next__()
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank17]:     batch = next(self.iterator)
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank17]:     out = next(self._iterator)
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank17]:     out[i] = next(self.iterators[i])
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank17]:     data = self._next_data()
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank23]:     results = self._run_stage()
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank23]:     self.fit_loop.run()
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank23]:     self.advance()
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank23]:     self.epoch_loop.run(self._data_fetcher)
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank23]:     self.advance(data_fetcher)
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank37]:     success, data = self._try_get_data()
[rank37]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank37]:     raise RuntimeError(
[rank37]: RuntimeError: DataLoader worker (pid(s) 16971, 16978) exited unexpectedly
[rank46]: Traceback (most recent call last):
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank46]:     data = self._data_queue.get(timeout=timeout)
[rank46]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank46]:     self.not_empty.wait(remaining)
[rank46]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank46]:     gotit = waiter.acquire(True, timeout)
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank46]:     _error_if_any_worker_fails()
[rank46]: RuntimeError: DataLoader worker (pid 9218) is killed by signal: Aborted. 

[rank46]: The above exception was the direct cause of the following exception:

[rank46]: Traceback (most recent call last):
[rank46]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank52]: Traceback (most recent call last):
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank52]:     data = self._data_queue.get(timeout=timeout)
[rank52]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank52]:     self.not_empty.wait(remaining)
[rank52]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank52]:     gotit = waiter.acquire(True, timeout)
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank52]:     _error_if_any_worker_fails()
[rank52]: RuntimeError: DataLoader worker (pid 454) is killed by signal: Aborted. 

[rank52]: The above exception was the direct cause of the following exception:

[rank52]: Traceback (most recent call last):
[rank52]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank50]:     idx, data = self._get_data()
[rank3]:     batch, _, __ = next(data_fetcher)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank3]:     batch = super().__next__()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank3]:     batch = next(self.iterator)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank3]:     out = next(self._iterator)
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank3]:     out[i] = next(self.iterators[i])
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank3]:     data = self._next_data()
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank17]:     idx, data = self._get_data()
[rank23]:     batch, _, __ = next(data_fetcher)
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank23]:     batch = super().__next__()
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank23]:     batch = next(self.iterator)
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank23]:     out = next(self._iterator)
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank23]:     out[i] = next(self.iterators[i])
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank23]:     data = self._next_data()
[rank46]:     main(
[rank46]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank46]:     trainer.fit(
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank46]:     call._call_and_handle_interrupt(
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank46]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank46]:     return function(*args, **kwargs)
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank46]:     self._run(model, ckpt_path=ckpt_path)
[rank52]:     main(
[rank52]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank52]:     trainer.fit(
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank52]:     call._call_and_handle_interrupt(
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank52]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank52]:     return function(*args, **kwargs)
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank52]:     self._run(model, ckpt_path=ckpt_path)
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank51]:     success, data = self._try_get_data()
[rank51]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank51]:     raise RuntimeError(
[rank51]: RuntimeError: DataLoader worker (pid(s) 15626, 15630) exited unexpectedly
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank3]:     idx, data = self._get_data()
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank17]:     success, data = self._try_get_data()
[rank17]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank17]:     raise RuntimeError(
[rank17]: RuntimeError: DataLoader worker (pid(s) 31611, 31618) exited unexpectedly
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank23]:     idx, data = self._get_data()
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank46]:     results = self._run_stage()
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank46]:     self.fit_loop.run()
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank46]:     self.advance()
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank46]:     self.epoch_loop.run(self._data_fetcher)
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank46]:     self.advance(data_fetcher)
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank52]:     results = self._run_stage()
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank52]:     self.fit_loop.run()
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank52]:     self.advance()
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank52]:     self.epoch_loop.run(self._data_fetcher)
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank52]:     self.advance(data_fetcher)
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank48]:     success, data = self._try_get_data()
[rank48]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank48]:     raise RuntimeError(
[rank48]: RuntimeError: DataLoader worker (pid(s) 15628, 15635) exited unexpectedly
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank3]:     success, data = self._try_get_data()
[rank3]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank3]:     raise RuntimeError(
[rank3]: RuntimeError: DataLoader worker (pid(s) 4404, 4406) exited unexpectedly
[rank15]: Traceback (most recent call last):
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank15]:     data = self._data_queue.get(timeout=timeout)
[rank15]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank15]:     self.not_empty.wait(remaining)
[rank15]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank15]:     gotit = waiter.acquire(True, timeout)
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank15]:     _error_if_any_worker_fails()
[rank15]: RuntimeError: DataLoader worker (pid 10878) is killed by signal: Aborted. 

[rank15]: The above exception was the direct cause of the following exception:

[rank15]: Traceback (most recent call last):
[rank15]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank23]:     success, data = self._try_get_data()
[rank23]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank23]:     raise RuntimeError(
[rank23]: RuntimeError: DataLoader worker (pid(s) 503, 508) exited unexpectedly
[rank39]: Traceback (most recent call last):
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank39]:     data = self._data_queue.get(timeout=timeout)
[rank39]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank39]:     self.not_empty.wait(remaining)
[rank39]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank39]:     gotit = waiter.acquire(True, timeout)
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank39]:     _error_if_any_worker_fails()
[rank39]: RuntimeError: DataLoader worker (pid 16970) is killed by signal: Aborted. 

[rank39]: The above exception was the direct cause of the following exception:

[rank39]: Traceback (most recent call last):
[rank39]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank46]:     batch, _, __ = next(data_fetcher)
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank46]:     batch = super().__next__()
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank46]:     batch = next(self.iterator)
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank46]:     out = next(self._iterator)
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank46]:     out[i] = next(self.iterators[i])
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank46]:     data = self._next_data()
[rank52]:     batch, _, __ = next(data_fetcher)
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank52]:     batch = super().__next__()
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank52]:     batch = next(self.iterator)
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank52]:     out = next(self._iterator)
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank52]:     out[i] = next(self.iterators[i])
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank52]:     data = self._next_data()
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank50]:     success, data = self._try_get_data()
[rank50]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank50]:     raise RuntimeError(
[rank50]: RuntimeError: DataLoader worker (pid(s) 15629, 15634) exited unexpectedly
[rank63]: Traceback (most recent call last):
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank63]:     data = self._data_queue.get(timeout=timeout)
[rank63]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank63]:     self.not_empty.wait(remaining)
[rank63]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank63]:     gotit = waiter.acquire(True, timeout)
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank63]:     _error_if_any_worker_fails()
[rank63]: RuntimeError: DataLoader worker (pid 10808) is killed by signal: Aborted. 

[rank63]: The above exception was the direct cause of the following exception:

[rank63]: Traceback (most recent call last):
[rank63]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank15]:     main(
[rank15]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank15]:     trainer.fit(
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank15]:     call._call_and_handle_interrupt(
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank15]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank15]:     return function(*args, **kwargs)
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank15]:     self._run(model, ckpt_path=ckpt_path)
[rank39]:     main(
[rank39]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank39]:     trainer.fit(
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank39]:     call._call_and_handle_interrupt(
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank39]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank39]:     return function(*args, **kwargs)
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank39]:     self._run(model, ckpt_path=ckpt_path)
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank46]:     idx, data = self._get_data()
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank52]:     idx, data = self._get_data()
[rank49]: Traceback (most recent call last):
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank49]:     data = self._data_queue.get(timeout=timeout)
[rank49]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank49]:     self.not_empty.wait(remaining)
[rank49]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank49]:     gotit = waiter.acquire(True, timeout)
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank49]:     _error_if_any_worker_fails()
[rank49]: RuntimeError: DataLoader worker (pid 15627) is killed by signal: Aborted. 

[rank49]: The above exception was the direct cause of the following exception:

[rank49]: Traceback (most recent call last):
[rank49]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank63]:     main(
[rank63]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank63]:     trainer.fit(
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank63]:     call._call_and_handle_interrupt(
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank63]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank63]:     return function(*args, **kwargs)
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank63]:     self._run(model, ckpt_path=ckpt_path)
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank15]:     results = self._run_stage()
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank15]:     self.fit_loop.run()
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank15]:     self.advance()
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank15]:     self.epoch_loop.run(self._data_fetcher)
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank15]:     self.advance(data_fetcher)
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank39]:     results = self._run_stage()
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank39]:     self.fit_loop.run()
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank39]:     self.advance()
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank39]:     self.epoch_loop.run(self._data_fetcher)
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank39]:     self.advance(data_fetcher)
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank46]:     success, data = self._try_get_data()
[rank46]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank46]:     raise RuntimeError(
[rank46]: RuntimeError: DataLoader worker (pid(s) 9218, 9226) exited unexpectedly
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank52]:     success, data = self._try_get_data()
[rank52]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank52]:     raise RuntimeError(
[rank52]: RuntimeError: DataLoader worker (pid(s) 454, 459) exited unexpectedly
[rank49]:     main(
[rank49]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank49]:     trainer.fit(
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank49]:     call._call_and_handle_interrupt(
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank49]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank49]:     return function(*args, **kwargs)
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank49]:     self._run(model, ckpt_path=ckpt_path)
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank63]:     results = self._run_stage()
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank63]:     self.fit_loop.run()
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank63]:     self.advance()
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank63]:     self.epoch_loop.run(self._data_fetcher)
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank63]:     self.advance(data_fetcher)
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank15]:     batch, _, __ = next(data_fetcher)
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank15]:     batch = super().__next__()
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank15]:     batch = next(self.iterator)
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank15]:     out = next(self._iterator)
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank15]:     out[i] = next(self.iterators[i])
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank15]:     data = self._next_data()
[rank39]:     batch, _, __ = next(data_fetcher)
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank39]:     batch = super().__next__()
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank39]:     batch = next(self.iterator)
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank39]:     out = next(self._iterator)
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank39]:     out[i] = next(self.iterators[i])
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank39]:     data = self._next_data()
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank49]:     results = self._run_stage()
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank49]:     self.fit_loop.run()
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank49]:     self.advance()
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank49]:     self.epoch_loop.run(self._data_fetcher)
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank49]:     self.advance(data_fetcher)
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank15]:     idx, data = self._get_data()
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank39]:     idx, data = self._get_data()
[rank49]:     batch, _, __ = next(data_fetcher)
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank49]:     batch = super().__next__()
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank49]:     batch = next(self.iterator)
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank49]:     out = next(self._iterator)
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank49]:     out[i] = next(self.iterators[i])
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank49]:     data = self._next_data()
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank15]:     success, data = self._try_get_data()
[rank15]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank15]:     raise RuntimeError(
[rank15]: RuntimeError: DataLoader worker (pid(s) 10878, 10881) exited unexpectedly
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank39]:     success, data = self._try_get_data()
[rank39]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank39]:     raise RuntimeError(
[rank39]: RuntimeError: DataLoader worker (pid(s) 16970, 16974) exited unexpectedly
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank49]:     idx, data = self._get_data()
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank49]:     success, data = self._try_get_data()
[rank49]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank49]:     raise RuntimeError(
[rank49]: RuntimeError: DataLoader worker (pid(s) 15627, 15632) exited unexpectedly
[rank63]:     batch, _, __ = next(data_fetcher)
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank63]:     batch = super().__next__()
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank63]:     batch = next(self.iterator)
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank63]:     out = next(self._iterator)
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank63]:     out[i] = next(self.iterators[i])
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank63]:     data = self._next_data()
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank63]:     idx, data = self._get_data()
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank63]:     success, data = self._try_get_data()
[rank63]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank63]:     raise RuntimeError(
[rank63]: RuntimeError: DataLoader worker (pid(s) 10808, 10812) exited unexpectedly
[rank25]: Traceback (most recent call last):
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank25]:     data = self._data_queue.get(timeout=timeout)
[rank25]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank25]:     self.not_empty.wait(remaining)
[rank25]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank25]:     gotit = waiter.acquire(True, timeout)
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank25]:     _error_if_any_worker_fails()
[rank25]: RuntimeError: DataLoader worker (pid 31294) is killed by signal: Aborted. 

[rank25]: The above exception was the direct cause of the following exception:

[rank25]: Traceback (most recent call last):
[rank25]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank25]:     main(
[rank25]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank25]:     trainer.fit(
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank25]:     call._call_and_handle_interrupt(
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank25]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank25]:     return function(*args, **kwargs)
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank25]:     self._run(model, ckpt_path=ckpt_path)
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank25]:     results = self._run_stage()
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank25]:     self.fit_loop.run()
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank25]:     self.advance()
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank25]:     self.epoch_loop.run(self._data_fetcher)
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank25]:     self.advance(data_fetcher)
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank25]:     batch, _, __ = next(data_fetcher)
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank25]:     batch = super().__next__()
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank25]:     batch = next(self.iterator)
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank25]:     out = next(self._iterator)
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank25]:     out[i] = next(self.iterators[i])
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank25]:     data = self._next_data()
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank25]:     idx, data = self._get_data()
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank25]:     success, data = self._try_get_data()
[rank25]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank25]:     raise RuntimeError(
[rank25]: RuntimeError: DataLoader worker (pid(s) 31287, 31294) exited unexpectedly
[rank18]: Traceback (most recent call last):
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank18]:     data = self._data_queue.get(timeout=timeout)
[rank18]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank18]:     self.not_empty.wait(remaining)
[rank18]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank18]:     gotit = waiter.acquire(True, timeout)
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank18]:     _error_if_any_worker_fails()
[rank18]: RuntimeError: DataLoader worker (pid 31612) is killed by signal: Aborted. 

[rank18]: The above exception was the direct cause of the following exception:

[rank18]: Traceback (most recent call last):
[rank18]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank18]:     main(
[rank18]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank18]:     trainer.fit(
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank18]:     call._call_and_handle_interrupt(
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank18]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank18]:     return function(*args, **kwargs)
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank18]:     self._run(model, ckpt_path=ckpt_path)
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank18]:     results = self._run_stage()
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank18]:     self.fit_loop.run()
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank18]:     self.advance()
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank18]:     self.epoch_loop.run(self._data_fetcher)
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank18]:     self.advance(data_fetcher)
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank18]:     batch, _, __ = next(data_fetcher)
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank18]:     batch = super().__next__()
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank18]:     batch = next(self.iterator)
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank18]:     out = next(self._iterator)
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank18]:     out[i] = next(self.iterators[i])
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank18]:     data = self._next_data()
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank18]:     idx, data = self._get_data()
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank18]:     success, data = self._try_get_data()
[rank18]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank18]:     raise RuntimeError(
[rank18]: RuntimeError: DataLoader worker (pid(s) 31612, 31615) exited unexpectedly
[rank19]: Traceback (most recent call last):
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank19]:     data = self._data_queue.get(timeout=timeout)
[rank19]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank19]:     self.not_empty.wait(remaining)
[rank19]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank19]:     gotit = waiter.acquire(True, timeout)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank19]:     _error_if_any_worker_fails()
[rank19]: RuntimeError: DataLoader worker (pid 31610) is killed by signal: Aborted. 

[rank19]: The above exception was the direct cause of the following exception:

[rank19]: Traceback (most recent call last):
[rank19]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank28]: Traceback (most recent call last):
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank28]:     data = self._data_queue.get(timeout=timeout)
[rank28]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank28]:     self.not_empty.wait(remaining)
[rank28]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank28]:     gotit = waiter.acquire(True, timeout)
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank28]:     _error_if_any_worker_fails()
[rank28]: RuntimeError: DataLoader worker (pid 17669) is killed by signal: Aborted. 

[rank28]: The above exception was the direct cause of the following exception:

[rank28]: Traceback (most recent call last):
[rank28]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank19]:     main(
[rank19]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank19]:     trainer.fit(
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank19]:     call._call_and_handle_interrupt(
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank19]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank19]:     return function(*args, **kwargs)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank19]:     self._run(model, ckpt_path=ckpt_path)
[rank28]:     main(
[rank28]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank28]:     trainer.fit(
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank28]:     call._call_and_handle_interrupt(
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank28]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank28]:     return function(*args, **kwargs)
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank28]:     self._run(model, ckpt_path=ckpt_path)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank19]:     results = self._run_stage()
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank19]:     self.fit_loop.run()
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank19]:     self.advance()
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank19]:     self.epoch_loop.run(self._data_fetcher)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank19]:     self.advance(data_fetcher)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank28]:     results = self._run_stage()
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank28]:     self.fit_loop.run()
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank28]:     self.advance()
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank28]:     self.epoch_loop.run(self._data_fetcher)
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank28]:     self.advance(data_fetcher)
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank27]: Traceback (most recent call last):
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank27]:     data = self._data_queue.get(timeout=timeout)
[rank27]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank27]:     self.not_empty.wait(remaining)
[rank27]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank27]:     gotit = waiter.acquire(True, timeout)
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank27]:     _error_if_any_worker_fails()
[rank27]: RuntimeError: DataLoader worker (pid 31285) is killed by signal: Aborted. 

[rank27]: The above exception was the direct cause of the following exception:

[rank27]: Traceback (most recent call last):
[rank27]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank19]:     batch, _, __ = next(data_fetcher)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank19]:     batch = super().__next__()
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank19]:     batch = next(self.iterator)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank19]:     out = next(self._iterator)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank19]:     out[i] = next(self.iterators[i])
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank19]:     data = self._next_data()
[rank27]:     main(
[rank27]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank27]:     trainer.fit(
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank27]:     call._call_and_handle_interrupt(
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank27]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank27]:     return function(*args, **kwargs)
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank27]:     self._run(model, ckpt_path=ckpt_path)
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank19]:     idx, data = self._get_data()
[rank13]: Traceback (most recent call last):
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank13]:     data = self._data_queue.get(timeout=timeout)
[rank13]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank13]:     self.not_empty.wait(remaining)
[rank13]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank13]:     gotit = waiter.acquire(True, timeout)
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank13]:     _error_if_any_worker_fails()
[rank13]: RuntimeError: DataLoader worker (pid 10879) is killed by signal: Aborted. 

[rank13]: The above exception was the direct cause of the following exception:

[rank13]: Traceback (most recent call last):
[rank13]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank27]:     results = self._run_stage()
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank27]:     self.fit_loop.run()
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank27]:     self.advance()
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank27]:     self.epoch_loop.run(self._data_fetcher)
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank27]:     self.advance(data_fetcher)
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank19]:     success, data = self._try_get_data()
[rank19]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank19]:     raise RuntimeError(
[rank19]: RuntimeError: DataLoader worker (pid(s) 31610, 31613) exited unexpectedly
[rank13]:     main(
[rank13]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank13]:     trainer.fit(
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank13]:     call._call_and_handle_interrupt(
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank13]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank13]:     return function(*args, **kwargs)
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank13]:     self._run(model, ckpt_path=ckpt_path)
[rank27]:     batch, _, __ = next(data_fetcher)
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank27]:     batch = super().__next__()
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank27]:     batch = next(self.iterator)
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank27]:     out = next(self._iterator)
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank27]:     out[i] = next(self.iterators[i])
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank27]:     data = self._next_data()
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank13]:     results = self._run_stage()
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank13]:     self.fit_loop.run()
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank13]:     self.advance()
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank13]:     self.epoch_loop.run(self._data_fetcher)
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank13]:     self.advance(data_fetcher)
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank28]:     batch, _, __ = next(data_fetcher)
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank28]:     batch = super().__next__()
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank28]:     batch = next(self.iterator)
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank28]:     out = next(self._iterator)
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank28]:     out[i] = next(self.iterators[i])
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank28]:     data = self._next_data()
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank27]:     idx, data = self._get_data()
[rank13]:     batch, _, __ = next(data_fetcher)
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank13]:     batch = super().__next__()
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank13]:     batch = next(self.iterator)
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank13]:     out = next(self._iterator)
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank13]:     out[i] = next(self.iterators[i])
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank13]:     data = self._next_data()
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank28]:     idx, data = self._get_data()
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank27]:     success, data = self._try_get_data()
[rank27]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank27]:     raise RuntimeError(
[rank27]: RuntimeError: DataLoader worker (pid(s) 31285, 31288) exited unexpectedly
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank13]:     idx, data = self._get_data()
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank28]:     success, data = self._try_get_data()
[rank28]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank28]:     raise RuntimeError(
[rank28]: RuntimeError: DataLoader worker (pid(s) 17669, 17672) exited unexpectedly
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank13]:     success, data = self._try_get_data()
[rank13]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank13]:     raise RuntimeError(
[rank13]: RuntimeError: DataLoader worker (pid(s) 10879, 10885) exited unexpectedly
[rank16]: Traceback (most recent call last):
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank16]:     data = self._data_queue.get(timeout=timeout)
[rank16]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank16]:     self.not_empty.wait(remaining)
[rank16]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank16]:     gotit = waiter.acquire(True, timeout)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank16]:     _error_if_any_worker_fails()
[rank16]: RuntimeError: DataLoader worker (pid 31619) is killed by signal: Aborted. 

[rank16]: The above exception was the direct cause of the following exception:

[rank16]: Traceback (most recent call last):
[rank16]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank16]:     main(
[rank16]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank16]:     trainer.fit(
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank16]:     call._call_and_handle_interrupt(
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank16]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank16]:     return function(*args, **kwargs)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank16]:     self._run(model, ckpt_path=ckpt_path)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank16]:     results = self._run_stage()
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank16]:     self.fit_loop.run()
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank16]:     self.advance()
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank16]:     self.epoch_loop.run(self._data_fetcher)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank16]:     self.advance(data_fetcher)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank5]: Traceback (most recent call last):
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank5]:     data = self._data_queue.get(timeout=timeout)
[rank5]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank5]:     self.not_empty.wait(remaining)
[rank5]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank5]:     gotit = waiter.acquire(True, timeout)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank5]:     _error_if_any_worker_fails()
[rank5]: RuntimeError: DataLoader worker (pid 2843) is killed by signal: Aborted. 

[rank5]: The above exception was the direct cause of the following exception:

[rank5]: Traceback (most recent call last):
[rank5]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank5]:     main(
[rank16]:     batch, _, __ = next(data_fetcher)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank16]:     batch = super().__next__()
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank16]:     batch = next(self.iterator)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank16]:     out = next(self._iterator)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank16]:     out[i] = next(self.iterators[i])
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank16]:     data = self._next_data()
[rank5]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank5]:     trainer.fit(
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank5]:     call._call_and_handle_interrupt(
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank5]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank5]:     return function(*args, **kwargs)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank5]:     self._run(model, ckpt_path=ckpt_path)
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank16]:     idx, data = self._get_data()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank5]:     results = self._run_stage()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank5]:     self.fit_loop.run()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank5]:     self.advance()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank5]:     self.epoch_loop.run(self._data_fetcher)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank5]:     self.advance(data_fetcher)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank16]:     success, data = self._try_get_data()
[rank16]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank16]:     raise RuntimeError(
[rank16]: RuntimeError: DataLoader worker (pid(s) 31609, 31619) exited unexpectedly
[rank5]:     batch, _, __ = next(data_fetcher)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank5]:     batch = super().__next__()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank5]:     batch = next(self.iterator)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank5]:     out = next(self._iterator)
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank5]:     out[i] = next(self.iterators[i])
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank5]:     data = self._next_data()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank5]:     idx, data = self._get_data()
[rank6]: Traceback (most recent call last):
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank6]:     data = self._data_queue.get(timeout=timeout)
[rank6]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank6]:     self.not_empty.wait(remaining)
[rank6]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank6]:     gotit = waiter.acquire(True, timeout)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank6]:     _error_if_any_worker_fails()
[rank6]: RuntimeError: DataLoader worker (pid 2842) is killed by signal: Aborted. 

[rank6]: The above exception was the direct cause of the following exception:

[rank6]: Traceback (most recent call last):
[rank6]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank6]:     main(
[rank6]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank6]:     trainer.fit(
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank6]:     call._call_and_handle_interrupt(
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank6]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank6]:     return function(*args, **kwargs)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank6]:     self._run(model, ckpt_path=ckpt_path)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank6]:     results = self._run_stage()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank6]:     self.fit_loop.run()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank6]:     self.advance()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank6]:     self.epoch_loop.run(self._data_fetcher)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank6]:     self.advance(data_fetcher)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank6]:     batch, _, __ = next(data_fetcher)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank6]:     batch = super().__next__()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank6]:     batch = next(self.iterator)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank6]:     out = next(self._iterator)
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank6]:     out[i] = next(self.iterators[i])
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank6]:     data = self._next_data()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank6]:     idx, data = self._get_data()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank5]:     success, data = self._try_get_data()
[rank5]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank5]:     raise RuntimeError(
[rank5]: RuntimeError: DataLoader worker (pid(s) 2843, 2849) exited unexpectedly
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank6]:     success, data = self._try_get_data()
[rank6]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank6]:     raise RuntimeError(
[rank6]: RuntimeError: DataLoader worker (pid(s) 2842, 2846) exited unexpectedly
[rank42]: Traceback (most recent call last):
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank42]:     data = self._data_queue.get(timeout=timeout)
[rank42]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank42]:     self.not_empty.wait(remaining)
[rank42]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank42]:     gotit = waiter.acquire(True, timeout)
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank42]:     _error_if_any_worker_fails()
[rank42]: RuntimeError: DataLoader worker (pid 31340) is killed by signal: Aborted. 

[rank42]: The above exception was the direct cause of the following exception:

[rank42]: Traceback (most recent call last):
[rank42]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank42]:     main(
[rank42]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank42]:     trainer.fit(
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank42]:     call._call_and_handle_interrupt(
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank42]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank42]:     return function(*args, **kwargs)
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank42]:     self._run(model, ckpt_path=ckpt_path)
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank42]:     results = self._run_stage()
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank42]:     self.fit_loop.run()
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank42]:     self.advance()
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank42]:     self.epoch_loop.run(self._data_fetcher)
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank42]:     self.advance(data_fetcher)
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank42]:     batch, _, __ = next(data_fetcher)
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank42]:     batch = super().__next__()
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank42]:     batch = next(self.iterator)
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank42]:     out = next(self._iterator)
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank42]:     out[i] = next(self.iterators[i])
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank42]:     data = self._next_data()
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank42]:     idx, data = self._get_data()
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank42]:     success, data = self._try_get_data()
[rank42]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank42]:     raise RuntimeError(
[rank42]: RuntimeError: DataLoader worker (pid(s) 31340, 31346) exited unexpectedly
[rank38]: Traceback (most recent call last):
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank38]:     data = self._data_queue.get(timeout=timeout)
[rank38]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank38]:     self.not_empty.wait(remaining)
[rank38]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank38]:     gotit = waiter.acquire(True, timeout)
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank38]:     _error_if_any_worker_fails()
[rank38]: RuntimeError: DataLoader worker (pid 16979) is killed by signal: Aborted. 

[rank38]: The above exception was the direct cause of the following exception:

[rank38]: Traceback (most recent call last):
[rank38]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank38]:     main(
[rank38]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank38]:     trainer.fit(
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank38]:     call._call_and_handle_interrupt(
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank38]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank38]:     return function(*args, **kwargs)
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank38]:     self._run(model, ckpt_path=ckpt_path)
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank38]:     results = self._run_stage()
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank38]:     self.fit_loop.run()
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank38]:     self.advance()
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank38]:     self.epoch_loop.run(self._data_fetcher)
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank38]:     self.advance(data_fetcher)
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank38]:     batch, _, __ = next(data_fetcher)
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank38]:     batch = super().__next__()
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank38]:     batch = next(self.iterator)
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank38]:     out = next(self._iterator)
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank38]:     out[i] = next(self.iterators[i])
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank38]:     data = self._next_data()
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank38]:     idx, data = self._get_data()
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank38]:     success, data = self._try_get_data()
[rank38]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank38]:     raise RuntimeError(
[rank38]: RuntimeError: DataLoader worker (pid(s) 16972, 16979) exited unexpectedly
[rank34]: Traceback (most recent call last):
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank34]:     data = self._data_queue.get(timeout=timeout)
[rank34]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank34]:     self.not_empty.wait(remaining)
[rank34]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank34]:     gotit = waiter.acquire(True, timeout)
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank34]:     _error_if_any_worker_fails()
[rank34]: RuntimeError: DataLoader worker (pid 18329) is killed by signal: Aborted. 

[rank34]: The above exception was the direct cause of the following exception:

[rank34]: Traceback (most recent call last):
[rank34]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank34]:     main(
[rank34]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank34]:     trainer.fit(
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank34]:     call._call_and_handle_interrupt(
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank34]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank34]:     return function(*args, **kwargs)
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank34]:     self._run(model, ckpt_path=ckpt_path)
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank34]:     results = self._run_stage()
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank34]:     self.fit_loop.run()
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank34]:     self.advance()
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank34]:     self.epoch_loop.run(self._data_fetcher)
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank34]:     self.advance(data_fetcher)
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank34]:     batch, _, __ = next(data_fetcher)
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank34]:     batch = super().__next__()
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank34]:     batch = next(self.iterator)
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank34]:     out = next(self._iterator)
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank34]:     out[i] = next(self.iterators[i])
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank34]:     data = self._next_data()
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank34]:     idx, data = self._get_data()
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank34]:     success, data = self._try_get_data()
[rank34]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank34]:     raise RuntimeError(
[rank34]: RuntimeError: DataLoader worker (pid(s) 18323, 18329) exited unexpectedly
[rank36]: Traceback (most recent call last):
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank36]:     data = self._data_queue.get(timeout=timeout)
[rank36]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank36]:     self.not_empty.wait(remaining)
[rank36]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank36]:     gotit = waiter.acquire(True, timeout)
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank36]:     _error_if_any_worker_fails()
[rank36]: RuntimeError: DataLoader worker (pid 16973) is killed by signal: Aborted. 

[rank36]: The above exception was the direct cause of the following exception:

[rank36]: Traceback (most recent call last):
[rank36]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank36]:     main(
[rank36]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank36]:     trainer.fit(
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank36]:     call._call_and_handle_interrupt(
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank36]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank36]:     return function(*args, **kwargs)
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank36]:     self._run(model, ckpt_path=ckpt_path)
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank36]:     results = self._run_stage()
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank36]:     self.fit_loop.run()
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank36]:     self.advance()
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank36]:     self.epoch_loop.run(self._data_fetcher)
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank36]:     self.advance(data_fetcher)
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank36]:     batch, _, __ = next(data_fetcher)
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank36]:     batch = super().__next__()
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank36]:     batch = next(self.iterator)
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank36]:     out = next(self._iterator)
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank36]:     out[i] = next(self.iterators[i])
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank36]:     data = self._next_data()
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank36]:     idx, data = self._get_data()
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank36]:     success, data = self._try_get_data()
[rank36]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank36]:     raise RuntimeError(
[rank36]: RuntimeError: DataLoader worker (pid(s) 16973, 16975) exited unexpectedly
[rank7]: Traceback (most recent call last):
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank7]:     data = self._data_queue.get(timeout=timeout)
[rank7]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank7]:     self.not_empty.wait(remaining)
[rank7]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank7]:     gotit = waiter.acquire(True, timeout)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank7]:     _error_if_any_worker_fails()
[rank7]: RuntimeError: DataLoader worker (pid 2840) is killed by signal: Aborted. 

[rank7]: The above exception was the direct cause of the following exception:

[rank7]: Traceback (most recent call last):
[rank7]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank7]:     main(
[rank7]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank7]:     trainer.fit(
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank7]:     call._call_and_handle_interrupt(
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank7]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank7]:     return function(*args, **kwargs)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank7]:     self._run(model, ckpt_path=ckpt_path)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank7]:     results = self._run_stage()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank7]:     self.fit_loop.run()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank7]:     self.advance()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank7]:     self.epoch_loop.run(self._data_fetcher)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank7]:     self.advance(data_fetcher)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank7]:     batch, _, __ = next(data_fetcher)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank7]:     batch = super().__next__()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank7]:     batch = next(self.iterator)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank7]:     out = next(self._iterator)
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank7]:     out[i] = next(self.iterators[i])
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank7]:     data = self._next_data()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank7]:     idx, data = self._get_data()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank7]:     success, data = self._try_get_data()
[rank7]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank7]:     raise RuntimeError(
[rank7]: RuntimeError: DataLoader worker (pid(s) 2840, 2844) exited unexpectedly
[rank24]: Traceback (most recent call last):
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank24]:     data = self._data_queue.get(timeout=timeout)
[rank24]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank24]:     self.not_empty.wait(remaining)
[rank24]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank24]:     gotit = waiter.acquire(True, timeout)
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank24]:     _error_if_any_worker_fails()
[rank24]: RuntimeError: DataLoader worker (pid 31284) is killed by signal: Aborted. 

[rank24]: The above exception was the direct cause of the following exception:

[rank24]: Traceback (most recent call last):
[rank24]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank24]:     main(
[rank24]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank24]:     trainer.fit(
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank24]:     call._call_and_handle_interrupt(
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank24]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank24]:     return function(*args, **kwargs)
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank24]:     self._run(model, ckpt_path=ckpt_path)
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank24]:     results = self._run_stage()
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank24]:     self.fit_loop.run()
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank24]:     self.advance()
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank24]:     self.epoch_loop.run(self._data_fetcher)
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank24]:     self.advance(data_fetcher)
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank24]:     batch, _, __ = next(data_fetcher)
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank24]:     batch = super().__next__()
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank24]:     batch = next(self.iterator)
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank24]:     out = next(self._iterator)
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank24]:     out[i] = next(self.iterators[i])
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank24]:     data = self._next_data()
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank24]:     idx, data = self._get_data()
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank24]:     success, data = self._try_get_data()
[rank24]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank24]:     raise RuntimeError(
[rank24]: RuntimeError: DataLoader worker (pid(s) 31284, 31290) exited unexpectedly
[rank4]: Traceback (most recent call last):
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank4]:     data = self._data_queue.get(timeout=timeout)
[rank4]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank4]:     self.not_empty.wait(remaining)
[rank4]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank4]:     gotit = waiter.acquire(True, timeout)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank4]:     _error_if_any_worker_fails()
[rank4]: RuntimeError: DataLoader worker (pid 2841) is killed by signal: Aborted. 

[rank4]: The above exception was the direct cause of the following exception:

[rank4]: Traceback (most recent call last):
[rank4]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank4]:     main(
[rank4]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank4]:     trainer.fit(
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank4]:     call._call_and_handle_interrupt(
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank4]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank4]:     return function(*args, **kwargs)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank4]:     self._run(model, ckpt_path=ckpt_path)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank4]:     results = self._run_stage()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank4]:     self.fit_loop.run()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank4]:     self.advance()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank4]:     self.epoch_loop.run(self._data_fetcher)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank4]:     self.advance(data_fetcher)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank4]:     batch, _, __ = next(data_fetcher)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank4]:     batch = super().__next__()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank4]:     batch = next(self.iterator)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank4]:     out = next(self._iterator)
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank4]:     out[i] = next(self.iterators[i])
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank4]:     data = self._next_data()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank4]:     idx, data = self._get_data()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank4]:     success, data = self._try_get_data()
[rank4]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank4]:     raise RuntimeError(
[rank4]: RuntimeError: DataLoader worker (pid(s) 2841, 2848) exited unexpectedly
[rank12]: Traceback (most recent call last):
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank12]:     data = self._data_queue.get(timeout=timeout)
[rank12]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank12]:     self.not_empty.wait(remaining)
[rank12]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank12]:     gotit = waiter.acquire(True, timeout)
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank12]:     _error_if_any_worker_fails()
[rank12]: RuntimeError: DataLoader worker (pid 10883) is killed by signal: Aborted. 

[rank12]: The above exception was the direct cause of the following exception:

[rank12]: Traceback (most recent call last):
[rank12]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank12]:     main(
[rank12]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank12]:     trainer.fit(
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank12]:     call._call_and_handle_interrupt(
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank12]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank12]:     return function(*args, **kwargs)
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank12]:     self._run(model, ckpt_path=ckpt_path)
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank12]:     results = self._run_stage()
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank12]:     self.fit_loop.run()
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank12]:     self.advance()
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank12]:     self.epoch_loop.run(self._data_fetcher)
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank12]:     self.advance(data_fetcher)
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank12]:     batch, _, __ = next(data_fetcher)
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank12]:     batch = super().__next__()
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank12]:     batch = next(self.iterator)
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank12]:     out = next(self._iterator)
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank12]:     out[i] = next(self.iterators[i])
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank12]:     data = self._next_data()
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank12]:     idx, data = self._get_data()
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank12]:     success, data = self._try_get_data()
[rank12]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank12]:     raise RuntimeError(
[rank12]: RuntimeError: DataLoader worker (pid(s) 10877, 10883) exited unexpectedly
[rank14]: Traceback (most recent call last):
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank14]:     data = self._data_queue.get(timeout=timeout)
[rank14]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank14]:     self.not_empty.wait(remaining)
[rank14]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank14]:     gotit = waiter.acquire(True, timeout)
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank14]:     _error_if_any_worker_fails()
[rank14]: RuntimeError: DataLoader worker (pid 10884) is killed by signal: Aborted. 

[rank14]: The above exception was the direct cause of the following exception:

[rank14]: Traceback (most recent call last):
[rank14]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank14]:     main(
[rank14]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank14]:     trainer.fit(
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank14]:     call._call_and_handle_interrupt(
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank14]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank14]:     return function(*args, **kwargs)
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank14]:     self._run(model, ckpt_path=ckpt_path)
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank14]:     results = self._run_stage()
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank14]:     self.fit_loop.run()
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank14]:     self.advance()
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank14]:     self.epoch_loop.run(self._data_fetcher)
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank14]:     self.advance(data_fetcher)
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank14]:     batch, _, __ = next(data_fetcher)
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank14]:     batch = super().__next__()
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank14]:     batch = next(self.iterator)
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank14]:     out = next(self._iterator)
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank14]:     out[i] = next(self.iterators[i])
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank14]:     data = self._next_data()
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank14]:     idx, data = self._get_data()
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank14]:     success, data = self._try_get_data()
[rank14]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank14]:     raise RuntimeError(
[rank14]: RuntimeError: DataLoader worker (pid(s) 10880, 10884) exited unexpectedly
[rank26]: Traceback (most recent call last):
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank26]:     data = self._data_queue.get(timeout=timeout)
[rank26]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank26]:     self.not_empty.wait(remaining)
[rank26]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank26]:     gotit = waiter.acquire(True, timeout)
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank26]:     _error_if_any_worker_fails()
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank26]:     _error_if_any_worker_fails()
[rank26]: RuntimeError: DataLoader worker (pid 31286) is killed by signal: Aborted. 

[rank26]: The above exception was the direct cause of the following exception:

[rank26]: Traceback (most recent call last):
[rank26]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank26]:     main(
[rank26]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank26]:     trainer.fit(
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank26]:     call._call_and_handle_interrupt(
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank26]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank26]:     return function(*args, **kwargs)
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank26]:     self._run(model, ckpt_path=ckpt_path)
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank26]:     results = self._run_stage()
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank26]:     self.fit_loop.run()
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank26]:     self.advance()
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank26]:     self.epoch_loop.run(self._data_fetcher)
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank26]:     self.advance(data_fetcher)
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank26]:     batch, _, __ = next(data_fetcher)
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank26]:     batch = super().__next__()
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank26]:     batch = next(self.iterator)
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank26]:     out = next(self._iterator)
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank26]:     out[i] = next(self.iterators[i])
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank26]:     data = self._next_data()
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank26]:     idx, data = self._get_data()
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank26]:     success, data = self._try_get_data()
[rank26]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank26]:     raise RuntimeError(
[rank26]: RuntimeError: DataLoader worker (pid(s) 31286, 31293) exited unexpectedly
[rank10]: Traceback (most recent call last):
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank10]:     data = self._data_queue.get(timeout=timeout)
[rank10]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank10]:     self.not_empty.wait(remaining)
[rank10]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank10]:     gotit = waiter.acquire(True, timeout)
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank10]:     _error_if_any_worker_fails()
[rank10]: RuntimeError: DataLoader worker (pid 10363) is killed by signal: Aborted. 

[rank10]: The above exception was the direct cause of the following exception:

[rank10]: Traceback (most recent call last):
[rank10]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank10]:     main(
[rank10]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank10]:     trainer.fit(
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank10]:     call._call_and_handle_interrupt(
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank10]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank10]:     return function(*args, **kwargs)
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank10]:     self._run(model, ckpt_path=ckpt_path)
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank10]:     results = self._run_stage()
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank10]:     self.fit_loop.run()
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank10]:     self.advance()
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank10]:     self.epoch_loop.run(self._data_fetcher)
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank10]:     self.advance(data_fetcher)
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank10]:     batch, _, __ = next(data_fetcher)
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank10]:     batch = super().__next__()
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank10]:     batch = next(self.iterator)
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank10]:     out = next(self._iterator)
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank10]:     out[i] = next(self.iterators[i])
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank10]:     data = self._next_data()
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank10]:     idx, data = self._get_data()
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank10]:     success, data = self._try_get_data()
[rank10]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank10]:     raise RuntimeError(
[rank10]: RuntimeError: DataLoader worker (pid(s) 10363, 10371) exited unexpectedly
[rank56]: Traceback (most recent call last):
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank56]:     data = self._data_queue.get(timeout=timeout)
[rank56]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank56]:     self.not_empty.wait(remaining)
[rank56]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank56]:     gotit = waiter.acquire(True, timeout)
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank56]:     _error_if_any_worker_fails()
[rank56]: RuntimeError: DataLoader worker (pid 29296) is killed by signal: Aborted. 

[rank56]: The above exception was the direct cause of the following exception:

[rank56]: Traceback (most recent call last):
[rank56]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank56]:     main(
[rank56]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank56]:     trainer.fit(
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank56]:     call._call_and_handle_interrupt(
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank56]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank56]:     return function(*args, **kwargs)
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank56]:     self._run(model, ckpt_path=ckpt_path)
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank56]:     results = self._run_stage()
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank56]:     self.fit_loop.run()
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank56]:     self.advance()
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank56]:     self.epoch_loop.run(self._data_fetcher)
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank56]:     self.advance(data_fetcher)
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank56]:     batch, _, __ = next(data_fetcher)
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank56]:     batch = super().__next__()
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank56]:     batch = next(self.iterator)
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank56]:     out = next(self._iterator)
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank56]:     out[i] = next(self.iterators[i])
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank56]:     data = self._next_data()
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank56]:     idx, data = self._get_data()
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank56]:     success, data = self._try_get_data()
[rank56]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank56]:     raise RuntimeError(
[rank56]: RuntimeError: DataLoader worker (pid(s) 29296, 29303) exited unexpectedly
[rank59]: Traceback (most recent call last):
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank59]:     data = self._data_queue.get(timeout=timeout)
[rank59]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank59]:     self.not_empty.wait(remaining)
[rank59]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank59]:     gotit = waiter.acquire(True, timeout)
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank59]:     _error_if_any_worker_fails()
[rank59]: RuntimeError: DataLoader worker (pid 29297) is killed by signal: Aborted. 

[rank59]: The above exception was the direct cause of the following exception:

[rank59]: Traceback (most recent call last):
[rank59]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank59]:     main(
[rank59]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank59]:     trainer.fit(
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank59]:     call._call_and_handle_interrupt(
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank59]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank59]:     return function(*args, **kwargs)
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank59]:     self._run(model, ckpt_path=ckpt_path)
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank59]:     results = self._run_stage()
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank59]:     self.fit_loop.run()
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank59]:     self.advance()
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank59]:     self.epoch_loop.run(self._data_fetcher)
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank59]:     self.advance(data_fetcher)
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank9]: Traceback (most recent call last):
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank9]:     data = self._data_queue.get(timeout=timeout)
[rank9]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank9]:     self.not_empty.wait(remaining)
[rank9]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank9]:     gotit = waiter.acquire(True, timeout)
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank9]:     _error_if_any_worker_fails()
[rank9]: RuntimeError: DataLoader worker (pid 10364) is killed by signal: Aborted. 

[rank9]: The above exception was the direct cause of the following exception:

[rank9]: Traceback (most recent call last):
[rank9]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank9]:     main(
[rank9]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank9]:     trainer.fit(
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank9]:     call._call_and_handle_interrupt(
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank9]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank9]:     return function(*args, **kwargs)
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank9]:     self._run(model, ckpt_path=ckpt_path)
[rank59]:     batch, _, __ = next(data_fetcher)
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank59]:     batch = super().__next__()
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank59]:     batch = next(self.iterator)
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank59]:     out = next(self._iterator)
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank59]:     out[i] = next(self.iterators[i])
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank59]:     data = self._next_data()
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank59]:     idx, data = self._get_data()
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank9]:     results = self._run_stage()
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank9]:     self.fit_loop.run()
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank9]:     self.advance()
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank9]:     self.epoch_loop.run(self._data_fetcher)
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank9]:     self.advance(data_fetcher)
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank59]:     success, data = self._try_get_data()
[rank59]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank59]:     raise RuntimeError(
[rank59]: RuntimeError: DataLoader worker (pid(s) 29297, 29300) exited unexpectedly
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank2]:     data = self._data_queue.get(timeout=timeout)
[rank2]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank2]:     self.not_empty.wait(remaining)
[rank2]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank2]:     gotit = waiter.acquire(True, timeout)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank2]:     _error_if_any_worker_fails()
[rank2]: RuntimeError: DataLoader worker (pid 4401) is killed by signal: Aborted. 

[rank2]: The above exception was the direct cause of the following exception:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank2]:     main(
[rank9]:     batch, _, __ = next(data_fetcher)
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank9]:     batch = super().__next__()
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank9]:     batch = next(self.iterator)
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank9]:     out = next(self._iterator)
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank9]:     out[i] = next(self.iterators[i])
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank9]:     data = self._next_data()
[rank45]: Traceback (most recent call last):
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank45]:     data = self._data_queue.get(timeout=timeout)
[rank45]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank45]:     self.not_empty.wait(remaining)
[rank45]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank45]:     gotit = waiter.acquire(True, timeout)
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank45]:     _error_if_any_worker_fails()
[rank45]: RuntimeError: DataLoader worker (pid 9224) is killed by signal: Aborted. 

[rank45]: The above exception was the direct cause of the following exception:

[rank45]: Traceback (most recent call last):
[rank45]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank2]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank2]:     trainer.fit(
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank2]:     call._call_and_handle_interrupt(
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank2]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank2]:     return function(*args, **kwargs)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank2]:     self._run(model, ckpt_path=ckpt_path)
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank9]:     idx, data = self._get_data()
[rank45]:     main(
[rank45]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank45]:     trainer.fit(
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank45]:     call._call_and_handle_interrupt(
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank45]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank45]:     return function(*args, **kwargs)
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank45]:     self._run(model, ckpt_path=ckpt_path)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank2]:     results = self._run_stage()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank2]:     self.fit_loop.run()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank2]:     self.advance()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank2]:     self.epoch_loop.run(self._data_fetcher)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank2]:     self.advance(data_fetcher)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank9]:     success, data = self._try_get_data()
[rank9]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank9]:     raise RuntimeError(
[rank9]: RuntimeError: DataLoader worker (pid(s) 10364, 10372) exited unexpectedly
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank45]:     results = self._run_stage()
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank45]:     self.fit_loop.run()
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank45]:     self.advance()
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank45]:     self.epoch_loop.run(self._data_fetcher)
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank45]:     self.advance(data_fetcher)
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank2]:     batch, _, __ = next(data_fetcher)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank2]:     batch = super().__next__()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank2]:     batch = next(self.iterator)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank2]:     out = next(self._iterator)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank2]:     out[i] = next(self.iterators[i])
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank2]:     data = self._next_data()
[rank45]:     batch, _, __ = next(data_fetcher)
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank45]:     batch = super().__next__()
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank45]:     batch = next(self.iterator)
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank45]:     out = next(self._iterator)
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank45]:     out[i] = next(self.iterators[i])
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank45]:     data = self._next_data()
[rank61]: Traceback (most recent call last):
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank61]:     data = self._data_queue.get(timeout=timeout)
[rank61]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank61]:     self.not_empty.wait(remaining)
[rank61]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank61]:     gotit = waiter.acquire(True, timeout)
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank61]:     _error_if_any_worker_fails()
[rank61]: RuntimeError: DataLoader worker (pid 10815) is killed by signal: Aborted. 

[rank61]: The above exception was the direct cause of the following exception:

[rank61]: Traceback (most recent call last):
[rank61]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank2]:     idx, data = self._get_data()
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank45]:     idx, data = self._get_data()
[rank61]:     main(
[rank61]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank61]:     trainer.fit(
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank61]:     call._call_and_handle_interrupt(
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank61]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank61]:     return function(*args, **kwargs)
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank61]:     self._run(model, ckpt_path=ckpt_path)
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank2]:     success, data = self._try_get_data()
[rank2]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank2]:     raise RuntimeError(
[rank2]: RuntimeError: DataLoader worker (pid(s) 4401, 4413) exited unexpectedly
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank45]:     success, data = self._try_get_data()
[rank45]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank45]:     raise RuntimeError(
[rank45]: RuntimeError: DataLoader worker (pid(s) 9221, 9224) exited unexpectedly
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank61]:     results = self._run_stage()
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank61]:     self.fit_loop.run()
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank61]:     self.advance()
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank61]:     self.epoch_loop.run(self._data_fetcher)
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank61]:     self.advance(data_fetcher)
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank61]:     batch, _, __ = next(data_fetcher)
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank61]:     batch = super().__next__()
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank61]:     batch = next(self.iterator)
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank61]:     out = next(self._iterator)
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank61]:     out[i] = next(self.iterators[i])
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank61]:     data = self._next_data()
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank61]:     idx, data = self._get_data()
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank61]:     success, data = self._try_get_data()
[rank61]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank61]:     raise RuntimeError(
[rank61]: RuntimeError: DataLoader worker (pid(s) 10811, 10815) exited unexpectedly
[rank62]: Traceback (most recent call last):
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank62]:     data = self._data_queue.get(timeout=timeout)
[rank62]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank62]:     self.not_empty.wait(remaining)
[rank62]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank62]:     gotit = waiter.acquire(True, timeout)
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank62]:     _error_if_any_worker_fails()
[rank62]: RuntimeError: DataLoader worker (pid 10816) is killed by signal: Aborted. 

[rank62]: The above exception was the direct cause of the following exception:

[rank62]: Traceback (most recent call last):
[rank62]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank62]:     main(
[rank62]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank62]:     trainer.fit(
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank62]:     call._call_and_handle_interrupt(
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank62]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank62]:     return function(*args, **kwargs)
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank62]:     self._run(model, ckpt_path=ckpt_path)
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank62]:     results = self._run_stage()
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank62]:     self.fit_loop.run()
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank62]:     self.advance()
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank62]:     self.epoch_loop.run(self._data_fetcher)
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank62]:     self.advance(data_fetcher)
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank62]:     batch, _, __ = next(data_fetcher)
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank62]:     batch = super().__next__()
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank62]:     batch = next(self.iterator)
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank62]:     out = next(self._iterator)
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank62]:     out[i] = next(self.iterators[i])
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank62]:     data = self._next_data()
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank62]:     idx, data = self._get_data()
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank62]:     success, data = self._try_get_data()
[rank62]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank62]:     raise RuntimeError(
[rank62]: RuntimeError: DataLoader worker (pid(s) 10809, 10816) exited unexpectedly
[rank58]: Traceback (most recent call last):
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank58]:     data = self._data_queue.get(timeout=timeout)
[rank58]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank58]:     self.not_empty.wait(remaining)
[rank58]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank58]:     gotit = waiter.acquire(True, timeout)
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank58]:     _error_if_any_worker_fails()
[rank58]: RuntimeError: DataLoader worker (pid 29298) is killed by signal: Aborted. 

[rank58]: The above exception was the direct cause of the following exception:

[rank58]: Traceback (most recent call last):
[rank58]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank58]:     main(
[rank58]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank58]:     trainer.fit(
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank58]:     call._call_and_handle_interrupt(
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank58]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank58]:     return function(*args, **kwargs)
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank58]:     self._run(model, ckpt_path=ckpt_path)
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank58]:     results = self._run_stage()
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank58]:     self.fit_loop.run()
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank58]:     self.advance()
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank58]:     self.epoch_loop.run(self._data_fetcher)
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank58]:     self.advance(data_fetcher)
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank58]:     batch, _, __ = next(data_fetcher)
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank58]:     batch = super().__next__()
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank58]:     batch = next(self.iterator)
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank58]:     out = next(self._iterator)
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank58]:     out[i] = next(self.iterators[i])
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank58]:     data = self._next_data()
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank58]:     idx, data = self._get_data()
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank58]:     success, data = self._try_get_data()
[rank58]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank58]:     raise RuntimeError(
[rank58]: RuntimeError: DataLoader worker (pid(s) 29298, 29304) exited unexpectedly
[rank35]: Traceback (most recent call last):
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank35]:     data = self._data_queue.get(timeout=timeout)
[rank35]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank35]:     self.not_empty.wait(remaining)
[rank35]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank35]:     gotit = waiter.acquire(True, timeout)
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank35]:     _error_if_any_worker_fails()
[rank35]: RuntimeError: DataLoader worker (pid 18322) is killed by signal: Aborted. 

[rank35]: The above exception was the direct cause of the following exception:

[rank35]: Traceback (most recent call last):
[rank35]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank35]:     main(
[rank35]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank35]:     trainer.fit(
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank35]:     call._call_and_handle_interrupt(
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank35]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank35]:     return function(*args, **kwargs)
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank35]:     self._run(model, ckpt_path=ckpt_path)
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank35]:     results = self._run_stage()
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank35]:     self.fit_loop.run()
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank35]:     self.advance()
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank35]:     self.epoch_loop.run(self._data_fetcher)
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank35]:     self.advance(data_fetcher)
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank35]:     batch, _, __ = next(data_fetcher)
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank35]:     batch = super().__next__()
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank35]:     batch = next(self.iterator)
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank35]:     out = next(self._iterator)
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank35]:     out[i] = next(self.iterators[i])
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank35]:     data = self._next_data()
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank35]:     idx, data = self._get_data()
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank35]:     success, data = self._try_get_data()
[rank35]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank35]:     raise RuntimeError(
[rank35]: RuntimeError: DataLoader worker (pid(s) 18322, 18326) exited unexpectedly
[rank40]: Traceback (most recent call last):
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank40]:     data = self._data_queue.get(timeout=timeout)
[rank40]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank40]:     self.not_empty.wait(remaining)
[rank40]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank40]:     gotit = waiter.acquire(True, timeout)
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank40]:     _error_if_any_worker_fails()
[rank40]: RuntimeError: DataLoader worker (pid 31337) is killed by signal: Aborted. 

[rank40]: The above exception was the direct cause of the following exception:

[rank40]: Traceback (most recent call last):
[rank40]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank40]:     main(
[rank40]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank40]:     trainer.fit(
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank40]:     call._call_and_handle_interrupt(
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank40]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank40]:     return function(*args, **kwargs)
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank40]:     self._run(model, ckpt_path=ckpt_path)
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank40]:     results = self._run_stage()
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank40]:     self.fit_loop.run()
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank40]:     self.advance()
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank40]:     self.epoch_loop.run(self._data_fetcher)
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank40]:     self.advance(data_fetcher)
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank40]:     batch, _, __ = next(data_fetcher)
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank40]:     batch = super().__next__()
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank40]:     batch = next(self.iterator)
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank40]:     out = next(self._iterator)
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank40]:     out[i] = next(self.iterators[i])
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank40]:     data = self._next_data()
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank40]:     idx, data = self._get_data()
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank40]:     success, data = self._try_get_data()
[rank40]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank40]:     raise RuntimeError(
[rank40]: RuntimeError: DataLoader worker (pid(s) 31337, 31342) exited unexpectedly
[rank47]: Traceback (most recent call last):
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank47]:     data = self._data_queue.get(timeout=timeout)
[rank47]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank47]:     self.not_empty.wait(remaining)
[rank47]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank47]:     gotit = waiter.acquire(True, timeout)
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank47]:     _error_if_any_worker_fails()
[rank47]: RuntimeError: DataLoader worker (pid 9222) is killed by signal: Aborted. 

[rank47]: The above exception was the direct cause of the following exception:

[rank47]: Traceback (most recent call last):
[rank47]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank47]:     main(
[rank47]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank47]:     trainer.fit(
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank47]:     call._call_and_handle_interrupt(
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank47]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank47]:     return function(*args, **kwargs)
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank47]:     self._run(model, ckpt_path=ckpt_path)
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank47]:     results = self._run_stage()
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank47]:     self.fit_loop.run()
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank47]:     self.advance()
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank47]:     self.epoch_loop.run(self._data_fetcher)
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank47]:     self.advance(data_fetcher)
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank47]:     batch, _, __ = next(data_fetcher)
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank47]:     batch = super().__next__()
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank47]:     batch = next(self.iterator)
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank47]:     out = next(self._iterator)
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank47]:     out[i] = next(self.iterators[i])
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank47]:     data = self._next_data()
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank47]:     idx, data = self._get_data()
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank47]:     success, data = self._try_get_data()
[rank47]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank47]:     raise RuntimeError(
[rank47]: RuntimeError: DataLoader worker (pid(s) 9219, 9222) exited unexpectedly
[rank43]: Traceback (most recent call last):
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank43]:     data = self._data_queue.get(timeout=timeout)
[rank43]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank43]:     self.not_empty.wait(remaining)
[rank43]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank43]:     gotit = waiter.acquire(True, timeout)
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank43]:     _error_if_any_worker_fails()
[rank43]: RuntimeError: DataLoader worker (pid 31338) is killed by signal: Aborted. 

[rank43]: The above exception was the direct cause of the following exception:

[rank43]: Traceback (most recent call last):
[rank43]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank43]:     main(
[rank43]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank43]:     trainer.fit(
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank43]:     call._call_and_handle_interrupt(
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank43]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank43]:     return function(*args, **kwargs)
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank43]:     self._run(model, ckpt_path=ckpt_path)
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank43]:     results = self._run_stage()
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank43]:     self.fit_loop.run()
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank43]:     self.advance()
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank43]:     self.epoch_loop.run(self._data_fetcher)
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank43]:     self.advance(data_fetcher)
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank43]:     batch, _, __ = next(data_fetcher)
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank43]:     batch = super().__next__()
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank43]:     batch = next(self.iterator)
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank43]:     out = next(self._iterator)
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank43]:     out[i] = next(self.iterators[i])
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank43]:     data = self._next_data()
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank43]:     idx, data = self._get_data()
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank43]:     success, data = self._try_get_data()
[rank43]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank43]:     raise RuntimeError(
[rank43]: RuntimeError: DataLoader worker (pid(s) 31338, 31341) exited unexpectedly
[rank41]: Traceback (most recent call last):
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank41]:     data = self._data_queue.get(timeout=timeout)
[rank41]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank41]:     self.not_empty.wait(remaining)
[rank41]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank41]:     gotit = waiter.acquire(True, timeout)
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank41]:     _error_if_any_worker_fails()
[rank41]: RuntimeError: DataLoader worker (pid 31339) is killed by signal: Aborted. 

[rank41]: The above exception was the direct cause of the following exception:

[rank41]: Traceback (most recent call last):
[rank41]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank41]:     main(
[rank41]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank41]:     trainer.fit(
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank41]:     call._call_and_handle_interrupt(
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank41]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank41]:     return function(*args, **kwargs)
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank41]:     self._run(model, ckpt_path=ckpt_path)
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank41]:     results = self._run_stage()
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank41]:     self.fit_loop.run()
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank41]:     self.advance()
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank41]:     self.epoch_loop.run(self._data_fetcher)
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank41]:     self.advance(data_fetcher)
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank41]:     batch, _, __ = next(data_fetcher)
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank41]:     batch = super().__next__()
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank41]:     batch = next(self.iterator)
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank41]:     out = next(self._iterator)
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank41]:     out[i] = next(self.iterators[i])
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank41]:     data = self._next_data()
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank41]:     idx, data = self._get_data()
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank41]:     success, data = self._try_get_data()
[rank41]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank41]:     raise RuntimeError(
[rank41]: RuntimeError: DataLoader worker (pid(s) 31339, 31344) exited unexpectedly
[rank57]: Traceback (most recent call last):
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank57]:     data = self._data_queue.get(timeout=timeout)
[rank57]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank57]:     self.not_empty.wait(remaining)
[rank57]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank57]:     gotit = waiter.acquire(True, timeout)
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank57]:     _error_if_any_worker_fails()
[rank57]: RuntimeError: DataLoader worker (pid 29299) is killed by signal: Aborted. 

[rank57]: The above exception was the direct cause of the following exception:

[rank57]: Traceback (most recent call last):
[rank57]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank57]:     main(
[rank57]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank57]:     trainer.fit(
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank57]:     call._call_and_handle_interrupt(
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank57]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank57]:     return function(*args, **kwargs)
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank57]:     self._run(model, ckpt_path=ckpt_path)
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank57]:     results = self._run_stage()
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank57]:     self.fit_loop.run()
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank57]:     self.advance()
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank57]:     self.epoch_loop.run(self._data_fetcher)
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank57]:     self.advance(data_fetcher)
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank57]:     batch, _, __ = next(data_fetcher)
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank57]:     batch = super().__next__()
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank57]:     batch = next(self.iterator)
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank57]:     out = next(self._iterator)
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank57]:     out[i] = next(self.iterators[i])
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank57]:     data = self._next_data()
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank57]:     idx, data = self._get_data()
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank57]:     success, data = self._try_get_data()
[rank57]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank57]:     raise RuntimeError(
[rank57]: RuntimeError: DataLoader worker (pid(s) 29299, 29302) exited unexpectedly
[rank31]: Traceback (most recent call last):
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank31]:     data = self._data_queue.get(timeout=timeout)
[rank31]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank31]:     self.not_empty.wait(remaining)
[rank31]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank31]:     gotit = waiter.acquire(True, timeout)
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank31]:     _error_if_any_worker_fails()
[rank31]: RuntimeError: DataLoader worker (pid 17670) is killed by signal: Aborted. 

[rank31]: The above exception was the direct cause of the following exception:

[rank31]: Traceback (most recent call last):
[rank31]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank31]:     main(
[rank31]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank31]:     trainer.fit(
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank31]:     call._call_and_handle_interrupt(
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank31]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank31]:     return function(*args, **kwargs)
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank31]:     self._run(model, ckpt_path=ckpt_path)
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank31]:     results = self._run_stage()
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank31]:     self.fit_loop.run()
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank31]:     self.advance()
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank31]:     self.epoch_loop.run(self._data_fetcher)
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank31]:     self.advance(data_fetcher)
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank31]:     batch, _, __ = next(data_fetcher)
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank31]:     batch = super().__next__()
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank31]:     batch = next(self.iterator)
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank31]:     out = next(self._iterator)
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank31]:     out[i] = next(self.iterators[i])
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank31]:     data = self._next_data()
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank31]:     idx, data = self._get_data()
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank31]:     success, data = self._try_get_data()
[rank31]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank31]:     raise RuntimeError(
[rank31]: RuntimeError: DataLoader worker (pid(s) 17670, 17671) exited unexpectedly
[rank33]: Traceback (most recent call last):
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank33]:     data = self._data_queue.get(timeout=timeout)
[rank33]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank33]:     self.not_empty.wait(remaining)
[rank33]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank33]:     gotit = waiter.acquire(True, timeout)
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank33]:     _error_if_any_worker_fails()
[rank33]: RuntimeError: DataLoader worker (pid 18331) is killed by signal: Aborted. 

[rank33]: The above exception was the direct cause of the following exception:

[rank33]: Traceback (most recent call last):
[rank33]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank33]:     main(
[rank33]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank33]:     trainer.fit(
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank33]:     call._call_and_handle_interrupt(
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank33]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank33]:     return function(*args, **kwargs)
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank33]:     self._run(model, ckpt_path=ckpt_path)
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank33]:     results = self._run_stage()
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank33]:     self.fit_loop.run()
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank33]:     self.advance()
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank33]:     self.epoch_loop.run(self._data_fetcher)
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank33]:     self.advance(data_fetcher)
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank33]:     batch, _, __ = next(data_fetcher)
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank33]:     batch = super().__next__()
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank33]:     batch = next(self.iterator)
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank33]:     out = next(self._iterator)
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank33]:     out[i] = next(self.iterators[i])
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank33]:     data = self._next_data()
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank33]:     idx, data = self._get_data()
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank33]:     success, data = self._try_get_data()
[rank33]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank33]:     raise RuntimeError(
[rank33]: RuntimeError: DataLoader worker (pid(s) 18324, 18331) exited unexpectedly
[rank32]: Traceback (most recent call last):
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank32]:     data = self._data_queue.get(timeout=timeout)
[rank32]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank32]:     self.not_empty.wait(remaining)
[rank32]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank32]:     gotit = waiter.acquire(True, timeout)
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank32]:     _error_if_any_worker_fails()
[rank32]: RuntimeError: DataLoader worker (pid 18328) is killed by signal: Aborted. 

[rank32]: The above exception was the direct cause of the following exception:

[rank32]: Traceback (most recent call last):
[rank32]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank32]:     main(
[rank32]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank32]:     trainer.fit(
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank32]:     call._call_and_handle_interrupt(
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank32]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank32]:     return function(*args, **kwargs)
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank32]:     self._run(model, ckpt_path=ckpt_path)
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank32]:     results = self._run_stage()
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank32]:     self.fit_loop.run()
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank32]:     self.advance()
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank32]:     self.epoch_loop.run(self._data_fetcher)
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank32]:     self.advance(data_fetcher)
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank32]:     batch, _, __ = next(data_fetcher)
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank32]:     batch = super().__next__()
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank32]:     batch = next(self.iterator)
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank32]:     out = next(self._iterator)
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank32]:     out[i] = next(self.iterators[i])
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank32]:     data = self._next_data()
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank32]:     idx, data = self._get_data()
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank32]:     success, data = self._try_get_data()
[rank32]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank32]:     raise RuntimeError(
[rank32]: RuntimeError: DataLoader worker (pid(s) 18321, 18328) exited unexpectedly
[rank30]: Traceback (most recent call last):
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank30]:     data = self._data_queue.get(timeout=timeout)
[rank30]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank30]:     self.not_empty.wait(remaining)
[rank30]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank30]:     gotit = waiter.acquire(True, timeout)
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank30]:     _error_if_any_worker_fails()
[rank30]: RuntimeError: DataLoader worker (pid 17675) is killed by signal: Aborted. 

[rank30]: The above exception was the direct cause of the following exception:

[rank30]: Traceback (most recent call last):
[rank30]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank30]:     main(
[rank30]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank30]:     trainer.fit(
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank30]:     call._call_and_handle_interrupt(
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank30]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank30]:     return function(*args, **kwargs)
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank30]:     self._run(model, ckpt_path=ckpt_path)
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank30]:     results = self._run_stage()
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank30]:     self.fit_loop.run()
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank30]:     self.advance()
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank30]:     self.epoch_loop.run(self._data_fetcher)
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank30]:     self.advance(data_fetcher)
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank30]:     batch, _, __ = next(data_fetcher)
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank30]:     batch = super().__next__()
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank30]:     batch = next(self.iterator)
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank30]:     out = next(self._iterator)
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank30]:     out[i] = next(self.iterators[i])
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank30]:     data = self._next_data()
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank30]:     idx, data = self._get_data()
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank30]:     success, data = self._try_get_data()
[rank30]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank30]:     raise RuntimeError(
[rank30]: RuntimeError: DataLoader worker (pid(s) 17668, 17675) exited unexpectedly
[rank29]: Traceback (most recent call last):
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1243, in _try_get_data
[rank29]:     data = self._data_queue.get(timeout=timeout)
[rank29]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/queue.py", line 180, in get
[rank29]:     self.not_empty.wait(remaining)
[rank29]:   File "/opt/apps/intel19/python3/3.9.2/lib/python3.9/threading.py", line 316, in wait
[rank29]:     gotit = waiter.acquire(True, timeout)
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank29]:     _error_if_any_worker_fails()
[rank29]: RuntimeError: DataLoader worker (pid 17667) is killed by signal: Aborted. 

[rank29]: The above exception was the direct cause of the following exception:

[rank29]: Traceback (most recent call last):
[rank29]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 100, in <module>
[rank29]:     main(
[rank29]:   File "/work2/10214/yu_yao/frontera/deepfaker/src/Demo_6.py", line 66, in main
[rank29]:     trainer.fit(
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 539, in fit
[rank29]:     call._call_and_handle_interrupt(
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py", line 46, in _call_and_handle_interrupt
[rank29]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
[rank29]:     return function(*args, **kwargs)
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 575, in _fit_impl
[rank29]:     self._run(model, ckpt_path=ckpt_path)
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 982, in _run
[rank29]:     results = self._run_stage()
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py", line 1026, in _run_stage
[rank29]:     self.fit_loop.run()
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
[rank29]:     self.advance()
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py", line 455, in advance
[rank29]:     self.epoch_loop.run(self._data_fetcher)
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 150, in run
[rank29]:     self.advance(data_fetcher)
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/training_epoch_loop.py", line 282, in advance
[rank29]:     batch, _, __ = next(data_fetcher)
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 134, in __next__
[rank29]:     batch = super().__next__()
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/loops/fetchers.py", line 61, in __next__
[rank29]:     batch = next(self.iterator)
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 341, in __next__
[rank29]:     out = next(self._iterator)
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/lightning/pytorch/utilities/combined_loader.py", line 78, in __next__
[rank29]:     out[i] = next(self.iterators[i])
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
[rank29]:     data = self._next_data()
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1448, in _next_data
[rank29]:     idx, data = self._get_data()
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1402, in _get_data
[rank29]:     success, data = self._try_get_data()
[rank29]:   File "/home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1256, in _try_get_data
[rank29]:     raise RuntimeError(
[rank29]: RuntimeError: DataLoader worker (pid(s) 17667, 17676) exited unexpectedly
srun: error: c198-062: tasks 28-31: Exited with exit code 1
srun: error: c199-101: tasks 52-55: Exited with exit code 1
srun: error: c199-092: tasks 48-51: Exited with exit code 1
srun: error: c198-071: tasks 32-35: Exited with exit code 1
srun: error: c198-052: tasks 20-23: Exited with exit code 1
srun: error: c198-032: tasks 4-7: Exited with exit code 1
srun: error: c197-111: tasks 1-3: Exited with exit code 1
srun: error: c199-091: tasks 44-47: Exited with exit code 1
srun: error: c198-042: tasks 12-15: Exited with exit code 1
srun: error: c198-072: tasks 36-39: Exited with exit code 1
srun: error: c198-091: tasks 40-43: Exited with exit code 1
srun: error: c198-041: tasks 8-11: Exited with exit code 1
srun: error: c199-102: tasks 56-59: Exited with exit code 1
srun: error: c198-061: tasks 24-27: Exited with exit code 1
srun: error: c199-111: tasks 60-63: Exited with exit code 1
srun: error: c198-051: tasks 16-19: Exited with exit code 1
[rank0]:[E131 12:17:44.699076025 ProcessGroupNCCL.cpp:542] [Rank 0] Collective WorkNCCL(SeqNum=24, OpType=ALLREDUCE, NumelIn=315478124, NumelOut=315478124, Timeout(ms)=1800000) raised the following async exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 192.168.44.187<33530> with status=5 opcode=129 len=16 vendor err 249 (Recv)
Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b781b72a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x2b77e78a5f80 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x2b77e78a61cc in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x213 (0x2b77e78adb93 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x2b77e78af61d in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x2b77d1a895c0 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x7ea5 (0x2b77c23f9ea5 in /usr/lib64/libpthread.so.0)
frame #7: clone + 0x6d (0x2b77c350cb0d in /usr/lib64/libc.so.6)

[rank0]:[E131 12:17:44.713862941 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 24, last enqueued NCCL work: 24, last completed NCCL work: 23.
[rank0]:[E131 12:17:44.713889360 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E131 12:17:44.713894697 ProcessGroupNCCL.cpp:636] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E131 12:17:44.763256405 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 192.168.44.187<33530> with status=5 opcode=129 len=16 vendor err 249 (Recv)
Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b781b72a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x2b77e78a5f80 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x2b77e78a61cc in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x213 (0x2b77e78adb93 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x2b77e78af61d in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x2b77d1a895c0 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x7ea5 (0x2b77c23f9ea5 in /usr/lib64/libpthread.so.0)
frame #7: clone + 0x6d (0x2b77c350cb0d in /usr/lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: NCCL error: remote process exited or there was a network error, NCCL version 2.21.5
ncclRemoteError: A call failed possibly due to a network error or a remote process exiting prematurely.
Last error:
NET/IB : Got completion from peer 192.168.44.187<33530> with status=5 opcode=129 len=16 vendor err 249 (Recv)
Exception raised from checkForNCCLErrorsInternal at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b781b72a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x2b77e78a5f80 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x2b77e78a61cc in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::watchdogHandler() + 0x213 (0x2b77e78adb93 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x2b77e78af61d in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #5: <unknown function> + 0x145c0 (0x2b77d1a895c0 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch.so)
frame #6: <unknown function> + 0x7ea5 (0x2b77c23f9ea5 in /usr/lib64/libpthread.so.0)
frame #7: clone + 0x6d (0x2b77c350cb0d in /usr/lib64/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x2b781b72a446 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe4271b (0x2b77e751c71b in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x2b77d1a895c0 in /home1/10214/yu_yao/.local/lib/python3.9/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x7ea5 (0x2b77c23f9ea5 in /usr/lib64/libpthread.so.0)
frame #4: clone + 0x6d (0x2b77c350cb0d in /usr/lib64/libc.so.6)

srun: error: c197-111: task 0: Aborted
